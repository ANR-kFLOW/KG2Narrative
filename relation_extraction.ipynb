{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "! pip install transformers evaluate rouge_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn import preprocessing\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data(data): #this is used to process the data for the tokenizer\n",
    "    context = data['context'].to_list() #First convert to a list\n",
    "    text_encodings = tokenizer(context, truncation=True, padding=True)\n",
    "\n",
    "    triplets = data['triplets'].to_list()\n",
    "    label_encodings = tokenizer(triplets, truncation=True, padding=True)\n",
    "    #new_data ={\"input_ids\":model_inputs[\"input_ids\"], \"labels\": labels[\"input_ids\"]}\n",
    "\n",
    "    return text_encodings, label_encodings\n",
    "\n",
    "class RebelDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_checkpoint = \"Babelscape/rebel-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 1\n",
    "data = pd.read_csv('rebel_format_v2.csv')\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "del data\n",
    "\n",
    "train_encodings, train_labels = preprocess_data(train_data)\n",
    "train_data = RebelDataset(train_encodings, train_labels)\n",
    "val_encodings, val_labels = preprocess_data(val_data)\n",
    "val_data = RebelDataset(val_encodings, val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-faro-relations\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=0.000025,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.1,\n",
    "    save_total_limit=3,\n",
    "    save_steps=300,\n",
    "    #load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    num_train_epochs=7,\n",
    "    predict_with_generate=True,\n",
    "    fp16 = True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def compute_rouge(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def check_format(data):\n",
    "    #This is used to check if the extraction worked\n",
    "\n",
    "    if len(data) != 3:\n",
    "      return ['wrong', 'wrong', 'wrong']\n",
    "    else:\n",
    "      return data\n",
    "\n",
    "def compute_f1(eval_pred):\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    metrics = {}\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=False)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "\n",
    "    pattern = r'>([^<]+)'\n",
    "    predictions = [check_format(re.findall(pattern, instance)) for instance in predictions]\n",
    "    try:\n",
    "      predictions = pd.DataFrame(predictions, columns =['Subject', 'Object', 'Relation'])\n",
    "    except:\n",
    "      print(predictions)\n",
    "\n",
    "\n",
    "    labels = [re.findall(pattern, instance) for instance in labels]\n",
    "    labels = pd.DataFrame(labels, columns =['Subject', 'Object', 'Relation'])\n",
    "\n",
    "    #F1 can only be computed on integer labels, so we need to encode them\n",
    "    le_subject= preprocessing.LabelEncoder()\n",
    "    le_subject.fit(predictions['Subject']) #Learn the encodings on the predictions for subject\n",
    "    le_subject_dict = dict(zip(le_subject.classes_, le_subject.transform(le_subject.classes_)))\n",
    "    del le_subject\n",
    "\n",
    "    #Encode in the same way, if it doesn't exist set to non existent number\n",
    "    predictions['Subject'] = predictions['Subject'].apply(lambda x: le_subject_dict.get(x, 999999999))\n",
    "    labels['Subject'] = labels['Subject'].apply(lambda x: le_subject_dict.get(x, 999999999))\n",
    "    metrics['f1_subject'] = f1_metric.compute(predictions=predictions['Subject'].to_list(), references=labels['Subject'].to_list(), average='macro')['f1']\n",
    "    del le_subject_dict\n",
    "\n",
    "    le_object= preprocessing.LabelEncoder()\n",
    "    le_object.fit(predictions['Object']) #Learn the encodings on the predictions for object\n",
    "    le_object_dict = dict(zip(le_object.classes_, le_object.transform(le_object.classes_)))\n",
    "    del le_object\n",
    "\n",
    "    predictions['Object'] = predictions['Object'].apply(lambda x: le_object_dict.get(x, 999999999))\n",
    "    labels['Object'] = labels['Object'].apply(lambda x: le_object_dict.get(x, 999999999))\n",
    "    metrics['f1_object'] = f1_metric.compute(predictions=predictions['Object'].to_list(), references=labels['Object'].to_list(), average='macro')['f1']\n",
    "    del le_object_dict\n",
    "\n",
    "    le_relations = preprocessing.LabelEncoder() #since there are a fixed number of classes no dict needs to be created\n",
    "    le_relations.fit(predictions['Relation']) #Learn the representation on the predictions for relations\n",
    "\n",
    "    predictions['Relation'] = le_relations.transform(predictions['Relation'])\n",
    "    labels['Relation'] = le_relations.transform(predictions['Relation'])\n",
    "    \n",
    "    f1 = f1_metric.compute(predictions=predictions['Relation'].to_list(), references=labels['Relation'].to_list(), average= None)['f1']\n",
    "\n",
    "    for relation, score in zip(le_relations.classes_, f1):\n",
    "        metrics[f'f1_{relation}'] = score\n",
    "\n",
    "    return metrics\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics= compute_f1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.save_model(\"finetuned_rebel\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"rebel_finetuned\", 'zip', \"finetuned_rebel\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The code underneath is for loading the model, and making inferences\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive('/content/drive/MyDrive/rebel_finetuned.zip', '/content/rebel_finetuned', 'zip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"rebel_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('Babelscape/rebel-large')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = [\"because the machine is old, it is unreliable\", \"many people have died in the storm\", \"now the preparation is complete, we can start again\",\n",
    "        \"the restrictions made sure less people got infected\", \"I am running everyday because i want to run a marathon\", \"the elevator is fixed, so i can go up again\",\n",
    "        \"There was a traffic jam, so i was late\", \"I broke my leg, so I can't run the marathon\", \"Since I failed the exam, I can't graduate\",\n",
    "        \"I did some shopping, because i want to cook later\", \"I shouldn't have said that, I did not mean that\", \"I wanted to say that\", \"I am planning on doing that later\",\n",
    "        \"I intend on doing that\"]\n",
    "encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# forward pass\n",
    "outputs = model.generate(**encoding, do_sample=True)\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for t, o in zip(text, decoded_output):\n",
    "  print(f\"{t} - {o}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for t, o in zip(text, decoded_output):\n",
    "  print(f\"{t} - {o}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VypmkohYUBm",
    "outputId": "9bc228dc-bf8c-4e6c-f36a-f3b5b5614cbc"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "because the machine is old, it is unreliable -  old  unreliable  cause\n",
      "many people have died in the storm -  died  storm  cause\n",
      "now the preparation is complete, we can start again -  preparation  start  enable\n",
      "the restrictions made sure less people got infected -  restrictions  got infected  prevent\n",
      "I am running everyday because i want to run a marathon -  running  want  cause\n",
      "the elevator is fixed, so i can go up again -  fixed  go  enable\n",
      "There was a traffic jam, so i was late -  jam  late  cause\n",
      "I broke my leg, so I can't run the marathon -  broke  marathon  prevent\n",
      "Since I failed the exam, I can't graduate -  passed  graduate  cause\n",
      "I did some shopping, because i want to cook later -  shopping  want  cause\n",
      "I shouldn't have said that, I did not mean that -  said  mean  cause\n",
      "I wanted to say that -  wanted  said  cause\n",
      "I am planning on doing that later -  planning  doing  cause\n",
      "I intend on doing that -  intend  doing  intend\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_5IGiTbRwdWU"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}