{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate the event and article knowledge graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import ast\n",
    "from resources import *\n",
    "import re\n",
    "\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "nif = Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "faro = Namespace(\"https://purl.org/faro/\")\n",
    "sem = Namespace(\"http://semanticweb.cs.vu.nl/2009/11/sem/\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "\n",
    "faro_classes = {'cause': faro.causes, 'enable': faro.enables, 'intend': faro.intends_to_cause, 'prevent': faro.prevents} #dict of faro definitions\n",
    "sem_props = {'http://www.wikidata.org/prop/direct/P710': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P664': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P112': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P17': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P276': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P625': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P131': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P30': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P585': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P580': sem.hasBeginTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P582': sem.hasEndTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P571': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P576': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P577': sem.hasTimeStamp,\n",
    "             'http://www.w3.org/2000/01/rdf-schema#label': 'what'}\n",
    "\n",
    "sem_classes = {sem.hasActor: sem.Actor,\n",
    "               sem.hasPlace: sem.Place,\n",
    "               sem.hasTime: sem.Time,\n",
    "               sem.hasBeginTimeStamp: sem.Time,\n",
    "               sem.hasEndTimeStamp: sem.Time,\n",
    "               sem.hasTimeStamp: sem.Time,\n",
    "               'what': sem.Event}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    This is used to remove the html codes from the text\n",
    "    :param text: The text to clean\n",
    "    :return: Cleaned text\n",
    "    '''\n",
    "\n",
    "    # Strip the last part of the text\n",
    "    index_of_last_occurence = text.rfind('</p><p>')\n",
    "    if index_of_last_occurence != -1:\n",
    "        text = text[:index_of_last_occurence]\n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \" \", text) # Strip all the special characters in the text\n",
    "\n",
    "    text = text.strip() #Remove the whitespace at the beginning, due to deletion\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/ASRAEL_data_full.csv')\n",
    "data['Text'] = data['Text'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when searching for event: http://www.wikidata.org/entity/Q100919128\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q113945893\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q105597606\n"
     ]
    }
   ],
   "source": [
    "#This converts the data into the right format, by removing uneccessary tokens in text and converting the wikidata link to text\n",
    "graph = Graph()\n",
    "\n",
    "event_mapping = {} #here the wikidata event urls and their names are saved\n",
    "failed_events= [] #Events that can't be found e.g. owl:sameAs need to be removed\n",
    "\n",
    "sparql = SPARQLWrapper(\n",
    "    \"https://query.wikidata.org/sparql\"\n",
    ")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "for event in data['Event'].unique().tolist():\n",
    "    event_ = f\"wd:{event.split('/')[-1]}\"\n",
    "\n",
    "    sparql.setQuery(\"\"\"\n",
    "    SELECT (?p as ?wiki_prop) (?o as ?result)\n",
    "    WHERE {{{\n",
    "\n",
    "        %s ?p ?temp.\n",
    "      ?temp rdfs:label ?o.\n",
    "      FILTER (lang(?o) = 'en') }\n",
    "      }\n",
    "\n",
    "      UNION\n",
    "\n",
    "      {\n",
    "       SELECT *\n",
    "       WHERE{\n",
    "        %s ?p ?o.\n",
    "         FILTER(lang(?o) = 'en' || lang(?o)='') }}\n",
    "     } \"\"\" % (event_, event_))\n",
    "\n",
    "    try:\n",
    "        result = sparql.queryAndConvert()\n",
    "        #event_name = ret['results']['bindings'][0]['item']['value']\n",
    "\n",
    "    except:\n",
    "        print(f\"Something went wrong when converting event: {event}\")\n",
    "\n",
    "    try:\n",
    "        event_data = pd.json_normalize(result[\"results\"][\"bindings\"])[['wiki_prop.value', 'result.value', 'result.datatype']]\n",
    "        event_data = event_data.rename(columns={\"wiki_prop.value\": \"property\", \"result.value\": \"value\", \"result.datatype\": \"datatype\"})\n",
    "        event_data = event_data.loc[event_data['property'].isin(sem_props.keys())].reset_index(drop=True) #Only keep the 4W attributes\n",
    "        event_data['property'] = event_data['property'].replace(sem_props)\n",
    "        event_name = event_data.loc[event_data['property'] == 'what']['value'].values[0] #This needs to be saved to map the wikidata urls to events\n",
    "        event_data = event_data[event_data.property != 'what'] #This row needs to be deleted for the loop\n",
    "        event_mapping[event] = event_name\n",
    "    except:\n",
    "        print(f\"Error when searching for event: {event}\")\n",
    "        failed_events.append(event)\n",
    "        continue\n",
    "\n",
    "    #event_uri = node_creation('', event_name, base_add='/event') #Generate the URI for the event\n",
    "    graph.add((URIRef(event), RDF.type, sem.Event)) #Create the event\n",
    "    graph.add((URIRef(event), RDF.value, Literal(event_name)))\n",
    "\n",
    "    for index, row in event_data.iterrows():\n",
    "        uri = node_creation('', row['value'], base_add='') #Generate the URI for the property\n",
    "        graph.add((uri, RDF.type, sem_classes[row['property']])) #Create the node for the property, and lookup its class\n",
    "        if pd.isna(row['datatype']) == False: #It has a declared datatype\n",
    "            graph.add((uri, RDF.value, Literal(row['value'], datatype=row['datatype'])))\n",
    "        else:\n",
    "            graph.add((uri, RDF.value, Literal(row['value']))) #Add the value of the relation to the graph\n",
    "        graph.add((URIRef(event), row['property'], uri)) #Connect the event to the property\n",
    "\n",
    "graph.serialize('Data/test/graphs/Event_graph_all.ttl', format='turtle')\n",
    "\n",
    "data= data[~data['Event'].isin(failed_events)] #remove the rows for which the event was not found\n",
    "#data['Event'] = data['Event'].map(event_mapping)\n",
    "#Check if this still is oke, it adds a list of events to the column event\n",
    "data = data.groupby(['URI','Identifier','Location', 'Time', 'Text']).agg({'Event': lambda x: list(x)}).reset_index(drop=False)\n",
    "data.to_csv('Data/test/ASRAEL_data_full_converted.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1122 [00:00<?, ?it/s]C:\\Users\\mike-\\Documents\\VU\\Eurecom\\KG_mapping\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1122 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform predictions again since the sentence number is needed for event resolution\n",
    "import pandas as pd\n",
    "from rebel_finetuning_faro import make_predictions\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('Data/test/ASRAEL_data_full_converted.csv')\n",
    "total_uri = []\n",
    "total_identifier = []\n",
    "total_location = []\n",
    "total_time = []\n",
    "total_event = []\n",
    "total_sentence_num = []\n",
    "total_sentences = []\n",
    "total_subject = []\n",
    "total_relation = []\n",
    "total_object = []\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "    for i, (sentence, prediction) in enumerate(zip(sentences, predictions)):\n",
    "\n",
    "        total_uri.append(row['URI'])\n",
    "        total_identifier.append(row['Identifier'])\n",
    "        total_location.append(row['Location'])\n",
    "        total_time.append(row['Time'])\n",
    "        total_event.append(row['Event'])\n",
    "        total_sentence_num.append(i)\n",
    "        total_sentences.append(sentence)\n",
    "        total_subject.append(prediction[0])\n",
    "        total_relation.append(prediction[1])\n",
    "        total_object.append(prediction[2])\n",
    "    break\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({'URI': total_uri, 'Identifier': total_identifier, 'Location': total_location, 'Time': total_time, 'Sentence_num': total_sentence_num, 'Sentence': total_sentences, 'Subject': total_subject, 'Relation': total_relation, 'Object': total_object, 'Event': total_event})\n",
    "\n",
    "new_data = new_data[~((new_data['Sentence'].str.len() < 25) & new_data['Sentence'].str.contains('/'))].reset_index(drop=True)\n",
    "\n",
    "new_data.to_csv('Data/test/final_data_with_predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19578/19578 [00:15<00:00, 1241.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=N98e1b3f780e649c59921af1aa1ddbe50 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "\n",
    "graph = Graph()\n",
    "graph.parse('Data/graphs/Event_graph_all.ttl')\n",
    "#data = pd.read_csv('Data/ASRAEL_data_full_converted.csv')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv')\n",
    "\n",
    "data_with_predictions = True #Set this to True if the cell above was executed\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\n",
    "    if not row.isnull().values.any(): #If nan is present skip the row\n",
    "\n",
    "        graph.add((URIRef(row['URI']), RDF.type, rnews.Article)) #Add the URI as article\n",
    "\n",
    "        identifier_uri = node_creation('', row['Identifier'], base_add='/identifier')\n",
    "        graph.add((URIRef(row['URI']), rnews.identifier, URIRef(identifier_uri))) #Link the PublicID to the article\n",
    "        graph.add((URIRef(identifier_uri), RDF.value, Literal(row['Identifier'])))\n",
    "\n",
    "        location_uri = node_creation('', row['Location'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentLocation, URIRef(location_uri)))\n",
    "        graph.add((URIRef(location_uri), RDF.value, Literal(row['Location'])))\n",
    "\n",
    "        time_uri = node_creation('', row['Time'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentReferenceTime, URIRef(time_uri)))\n",
    "        graph.add((URIRef(time_uri), RDF.value, Literal(row['Time'])))\n",
    "\n",
    "        if data_with_predictions == False: #Make the predictions\n",
    "\n",
    "            from rebel_finetuning_faro import make_predictions\n",
    "\n",
    "            sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "            for sentence, prediction in zip(sentences, predictions):\n",
    "\n",
    "                if prediction[1] in faro_classes:\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                    graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                    graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                    graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                    subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                    graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                    object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                    graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                    graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation betwee NERs\n",
    "                else:\n",
    "                    continue\n",
    "        else: # The data already contains the predictions\n",
    "            sentence = row['Sentence']\n",
    "            prediction = (row['Subject'], row['Relation'], row['Object'])\n",
    "\n",
    "            if prediction[1] in faro_classes:\n",
    "                sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((sentence_uri, nif.word, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((subject_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                graph.add((sentence_uri, nif.word, object_uri)) #Add the object to the sentence\n",
    "                graph.add((object_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation betwee NERs\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for event in ast.literal_eval(row['Event']): #Link the article to the corresponding event\n",
    "            #event_uri = node_creation('', event_name, base_add='/event')\n",
    "            graph.add((URIRef(row['URI']), schema.about, URIRef(event)))\n",
    "\n",
    "graph.serialize('Data/graphs/updated_ontology/event_article_graph_complete.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Used for drawing the graph\n",
    "import networkx as nx\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "\n",
    "nx_graph = rdflib_to_networkx_multidigraph(graph)\n",
    "pos = nx.spring_layout(nx_graph, scale=2)\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(nx_graph, 'r')\n",
    "nx.draw_networkx_edge_labels(nx_graph, pos, edge_labels=edge_labels)\n",
    "nx.draw(nx_graph, with_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge same entities\n",
    "### Entity coreference resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# This code will load the clusters from a text file, and merge the nodes in the cluster together\n",
    "import ast\n",
    "from resources import node_creation\n",
    "import pandas as pd\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = 'Data/cluster_data/output_all/'\n",
    "cluster_dirs = os.listdir(base_path)\n",
    "cluster_docs = [base_path + dir +'/event_clusters.txt' for dir in cluster_dirs if os.path.isdir(base_path + dir)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [01:40<00:00,  1.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=N11d3e619a4e4447cbeb1971e81fc8b40 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph()\n",
    "graph.parse('Data/graphs/updated_ontology/event_article_graph_complete.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv') #Load the original dataset\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "\n",
    "for doc in tqdm(cluster_docs):\n",
    "\n",
    "    with open(doc) as f: #Open the generated cluster file\n",
    "        cluster_doc = f.readlines()\n",
    "\n",
    "    for cluster in cluster_doc:\n",
    "\n",
    "        if cluster.startswith('['):\n",
    "            cluster = ast.literal_eval(cluster)\n",
    "\n",
    "            if len(cluster) != 1:\n",
    "                prev_mention_uri = None\n",
    "\n",
    "                for i, mention in enumerate(cluster):\n",
    "                    mention = mention.split('_')\n",
    "\n",
    "                    sentence = data[(data['URI'] == mention[1]) & (data['Sentence_num'] == int(mention[2]))]['Sentence'].values[0]\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "\n",
    "\n",
    "                    #determine subject or object\n",
    "                    if data[(data['URI'] == mention[1]) & (data['Sentence_num'] == int(mention[2]))]['Subject'].values[0] == mention[0]:\n",
    "                        mention_uri = node_creation('', mention[0] + str(sentence_uri), base_add='/subject')\n",
    "\n",
    "                    else:\n",
    "                        mention_uri = node_creation('', mention[0] + str(sentence_uri), base_add='/object')\n",
    "\n",
    "                    #print(mention_uri)\n",
    "\n",
    "                    if i != 0:\n",
    "                        graph.add((prev_mention_uri, owl.sameAs, mention_uri))\n",
    "\n",
    "\n",
    "                    prev_mention_uri = mention_uri\n",
    "graph.serialize('Data/graphs/updated_ontology/event_article_graph_complete_merged.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select most relevant information from the graph\n",
    "### Lookup the values of the selected nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Graph, URIRef\n",
    "from resources import uri_validator, mapping_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"2021-02-11T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-11T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-11T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-14T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-14T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-01T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-01T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-01T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-07T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-07T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-07T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n",
      "\"2021-02-07T00:00:00+00:00\"^^<http://www.w3.org/2001/XMLSchema#dateTime> does not look like a valid URI, trying to serialize this will break.\n"
     ]
    }
   ],
   "source": [
    "# Lookup the values of the selected nodes\n",
    "\n",
    "selected_nodes = pd.read_csv('Data/subgraph/10-subgraph.csv', index_col=0)\n",
    "graph = Graph()\n",
    "graph.parse(\"Data/graphs/event_article_graph_complete_merged.ttl\")\n",
    "\n",
    "subj_values = []\n",
    "obj_values = []\n",
    "\n",
    "subj_query = prepareQuery(\"\"\"\n",
    "    SELECT ?subj_value Where{\n",
    "\n",
    "    ?subject rdf:value ?subj_value.\n",
    "\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "\n",
    "obj_query = prepareQuery(\"\"\"\n",
    "    SELECT ?obj_value Where{\n",
    "\n",
    "    ?object rdf:value ?obj_value.\n",
    "\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "\n",
    "for index, row in selected_nodes.iterrows():\n",
    "\n",
    "\n",
    "    subj_node = URIRef(row['subject'])\n",
    "    obj_node = URIRef(row['object'])\n",
    "\n",
    "    subj_qres = graph.query(subj_query, initBindings={\"subject\": subj_node})\n",
    "    obj_qres = graph.query(obj_query, initBindings={\"object\": obj_node})\n",
    "\n",
    "    if len(subj_qres) != 0:\n",
    "        for row_result in subj_qres:\n",
    "\n",
    "            subj_values.append(row_result[0].value)\n",
    "            break\n",
    "\n",
    "    elif uri_validator(row['subject']) == False:\n",
    "        subj_values.append(row['subject'])\n",
    "\n",
    "    else:\n",
    "        subj_values.append(None)\n",
    "\n",
    "\n",
    "    if len(obj_qres) != 0:\n",
    "        for row_result in obj_qres:\n",
    "\n",
    "            obj_values.append(row_result[0].value)\n",
    "            break\n",
    "\n",
    "    elif uri_validator(row['object']) == False:\n",
    "        obj_values.append(row['object'])\n",
    "\n",
    "    else:\n",
    "        obj_values.append(None)\n",
    "\n",
    "selected_nodes['subject_values'] = subj_values\n",
    "selected_nodes['object_values'] = obj_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating the correct format for JointGT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def gen_jointgt_input_format(data, output_file, subj_col= 'subject_values', rel_col= 'predicate', obj_col= 'object_values', sent_col = None, single_event = True):\n",
    "    mapping_ids = mapping_dict(data[subj_col].to_list()) #Generate unique ID's per word\n",
    "\n",
    "    full_data = []\n",
    "    kbs = {}\n",
    "    for index, row in data.iterrows():\n",
    "        subj_id = f\"W{mapping_ids[row[subj_col]]}\"\n",
    "        relation = row[rel_col].split('/')[-1]\n",
    "        kbs[subj_id] = [row[subj_col], row[subj_col], [[relation, str(row[obj_col])]]]\n",
    "\n",
    "        if single_event == False:\n",
    "            json_dict = {\"id\": index,\n",
    "                         \"kbs\": kbs,\n",
    "                         \"text\": [row[sent_col]]}\n",
    "            full_data.append(json_dict)\n",
    "            kbs = {}\n",
    "\n",
    "\n",
    "    if single_event:\n",
    "        full_data = {\"id\": 1,\n",
    "                     \"kbs\": kbs,\n",
    "                     \"text\": [\"test\"]}\n",
    "\n",
    "    with open(output_file, \"w\") as json_out:\n",
    "\n",
    "        json.dump(full_data, json_out, indent = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "#Now convert rebel relationship data to JointGT input format\n",
    "\n",
    "relation_data = pd.read_csv('Data/rebel_v2/data/new_split/train.csv', index_col=0)\n",
    "gen_jointgt_input_format(relation_data, 'relation_dataset_jointgt.json', 'trigger1', 'label', 'trigger2', 'sentence', single_event= False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}