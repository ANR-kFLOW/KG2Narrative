{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate the event and article knowledge graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import ast\n",
    "from resources import *\n",
    "import re\n",
    "\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "nif = Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "faro = Namespace(\"https://purl.org/faro/\")\n",
    "sem = Namespace(\"http://semanticweb.cs.vu.nl/2009/11/sem/\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "\n",
    "faro_classes = {'cause': faro.causes, 'enable': faro.enables, 'intend': faro.intends_to_cause, 'prevent': faro.prevents} #dict of faro definitions\n",
    "sem_props = {'http://www.wikidata.org/prop/direct/P710': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P664': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P112': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P17': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P276': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P625': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P131': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P30': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P585': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P580': sem.hasBeginTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P582': sem.hasEndTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P571': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P576': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P577': sem.hasTimeStamp,\n",
    "             'http://www.w3.org/2000/01/rdf-schema#label': 'what'}\n",
    "\n",
    "sem_classes = {sem.hasActor: sem.Actor,\n",
    "               sem.hasPlace: sem.Place,\n",
    "               sem.hasTime: sem.Time,\n",
    "               sem.hasBeginTimeStamp: sem.Time,\n",
    "               sem.hasEndTimeStamp: sem.Time,\n",
    "               sem.hasTimeStamp: sem.Time,\n",
    "               'what': sem.Event}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from resources import clean_text\n",
    "data = pd.read_csv('Data/ASRAEL_data_full.csv')\n",
    "data['Text'] = data['Text'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when searching for event: http://www.wikidata.org/entity/Q100919128\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q113945893\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q105597606\n"
     ]
    }
   ],
   "source": [
    "#This converts the data into the right format, by removing uneccessary tokens in text and converting the wikidata link to text\n",
    "from resources import uri_validator\n",
    "graph = Graph()\n",
    "\n",
    "event_mapping = {} #here the wikidata event urls and their names are saved\n",
    "failed_events= [] #Events that can't be found e.g. owl:sameAs need to be removed\n",
    "\n",
    "sparql = SPARQLWrapper(\n",
    "    \"https://query.wikidata.org/sparql\"\n",
    ")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "for event in data['Event'].unique().tolist():\n",
    "    event_ = f\"wd:{event.split('/')[-1]}\"\n",
    "\n",
    "    sparql.setQuery(\"\"\"\n",
    "    SELECT (?p as ?wiki_prop) (?o as ?result)\n",
    "    WHERE {{{\n",
    "\n",
    "        %s ?p ?temp.\n",
    "      ?temp rdfs:label ?o.\n",
    "      FILTER (lang(?o) = 'en') }\n",
    "      }\n",
    "\n",
    "      UNION\n",
    "\n",
    "      {\n",
    "       SELECT *\n",
    "       WHERE{\n",
    "        %s ?p ?o.\n",
    "         FILTER(lang(?o) = 'en' || lang(?o)='') }}\n",
    "     } \"\"\" % (event_, event_))\n",
    "\n",
    "    try:\n",
    "        result = sparql.queryAndConvert()\n",
    "        #event_name = ret['results']['bindings'][0]['item']['value']\n",
    "\n",
    "    except:\n",
    "        print(f\"Something went wrong when converting event: {event}\")\n",
    "\n",
    "    try:\n",
    "        event_data = pd.json_normalize(result[\"results\"][\"bindings\"])[['wiki_prop.value', 'result.value', 'result.datatype']]\n",
    "        event_data = event_data.rename(columns={\"wiki_prop.value\": \"property\", \"result.value\": \"value\", \"result.datatype\": \"datatype\"})\n",
    "        event_data = event_data.loc[event_data['property'].isin(sem_props.keys())].reset_index(drop=True) #Only keep the 4W attributes\n",
    "        event_data['property'] = event_data['property'].replace(sem_props)\n",
    "        event_name = event_data.loc[event_data['property'] == 'what']['value'].values[0] #This needs to be saved to map the wikidata urls to events\n",
    "        event_data = event_data[event_data.property != 'what'] #This row needs to be deleted for the loop\n",
    "        event_mapping[event] = event_name\n",
    "    except:\n",
    "        print(f\"Error when searching for event: {event}\")\n",
    "        failed_events.append(event)\n",
    "        continue\n",
    "\n",
    "    #event_uri = node_creation('', event_name, base_add='/event') #Generate the URI for the event\n",
    "    graph.add((URIRef(event), RDF.type, sem.Event)) #Create the event\n",
    "    graph.add((URIRef(event), RDF.value, Literal(event_name)))\n",
    "\n",
    "    for index, row in event_data.iterrows():\n",
    "        uri = node_creation('', row['value'], base_add='') #Generate the URI for the property\n",
    "        if uri_validator(uri) == False:\n",
    "            print(f\"Found issue, generated link is not an uri:\\n{uri}\")\n",
    "        graph.add((uri, RDF.type, sem_classes[row['property']])) #Create the node for the property, and lookup its class\n",
    "        if pd.isna(row['datatype']) == False: #It has a declared datatype\n",
    "            graph.add((uri, RDF.value, Literal(row['value'], datatype=row['datatype'])))\n",
    "        else:\n",
    "            graph.add((uri, RDF.value, Literal(row['value']))) #Add the value of the relation to the graph\n",
    "        graph.add((URIRef(event), row['property'], uri)) #Connect the event to the property\n",
    "\n",
    "graph.serialize('Data/graphs/final_generated/Event_graph_all.ttl', format='turtle')\n",
    "\n",
    "data= data[~data['Event'].isin(failed_events)] #remove the rows for which the event was not found\n",
    "#data['Event'] = data['Event'].map(event_mapping)\n",
    "#Check if this still is oke, it adds a list of events to the column event\n",
    "data = data.groupby(['URI','Identifier','Location', 'Time', 'Text']).agg({'Event': lambda x: list(x)}).reset_index(drop=False)\n",
    "data.to_csv('Data/dataset_final_generated/ASRAEL_data_full_converted.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1122 [00:00<?, ?it/s]C:\\Users\\mike-\\Documents\\VU\\Eurecom\\KG_mapping\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1122 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform predictions, execute cell below to combine predictions and generate the graph at once.\n",
    "import pandas as pd\n",
    "from rebel_finetuning_faro import make_predictions\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('Data/dataset_final_generated/ASRAEL_data_full_converted.csv')\n",
    "total_uri = []\n",
    "total_identifier = []\n",
    "total_location = []\n",
    "total_time = []\n",
    "total_event = []\n",
    "total_sentence_num = []\n",
    "total_sentences = []\n",
    "total_subject = []\n",
    "total_relation = []\n",
    "total_object = []\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "    for i, (sentence, prediction) in enumerate(zip(sentences, predictions)):\n",
    "\n",
    "        total_uri.append(row['URI'])\n",
    "        total_identifier.append(row['Identifier'])\n",
    "        total_location.append(row['Location'])\n",
    "        total_time.append(row['Time'])\n",
    "        total_event.append(row['Event'])\n",
    "        total_sentence_num.append(i)\n",
    "        total_sentences.append(sentence)\n",
    "        total_subject.append(prediction[0])\n",
    "        total_relation.append(prediction[1])\n",
    "        total_object.append(prediction[2])\n",
    "    break\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({'URI': total_uri, 'Identifier': total_identifier, 'Location': total_location, 'Time': total_time, 'Sentence_num': total_sentence_num, 'Sentence': total_sentences, 'Subject': total_subject, 'Relation': total_relation, 'Object': total_object, 'Event': total_event})\n",
    "\n",
    "new_data = new_data[~((new_data['Sentence'].str.len() < 25) & new_data['Sentence'].str.contains('/'))].reset_index(drop=True)\n",
    "\n",
    "new_data.to_csv('Data/final_generated/final_data_with_predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19578/19578 [00:25<00:00, 754.16it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=N6eaa17f7cbe443be835e69bcc063e464 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "\n",
    "graph = Graph()\n",
    "graph.parse('Data/graphs/final_generated/Event_graph_all.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv')\n",
    "\n",
    "data_with_predictions = True #Set this to True if the cell above was executed\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\n",
    "    if not row.isnull().values.any(): #If nan is present skip the row\n",
    "\n",
    "        graph.add((URIRef(row['URI']), RDF.type, rnews.Article)) #Add the URI as article\n",
    "\n",
    "        identifier_uri = node_creation('', row['Identifier'], base_add='/identifier')\n",
    "        graph.add((URIRef(row['URI']), rnews.identifier, URIRef(identifier_uri))) #Link the PublicID to the article\n",
    "        graph.add((URIRef(identifier_uri), RDF.value, Literal(row['Identifier'])))\n",
    "\n",
    "        location_uri = node_creation('', row['Location'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentLocation, URIRef(location_uri)))\n",
    "        graph.add((URIRef(location_uri), RDF.value, Literal(row['Location'])))\n",
    "        graph.add((URIRef(location_uri), RDF.type, schema.Place))\n",
    "\n",
    "        time_uri = node_creation('', row['Time'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentReferenceTime, URIRef(time_uri)))\n",
    "        graph.add((URIRef(time_uri), RDF.value, Literal(row['Time'])))\n",
    "        graph.add((URIRef(time_uri), RDF.type, schema.Time))\n",
    "\n",
    "        if data_with_predictions == False: #Make the predictions\n",
    "\n",
    "            from rebel_finetuning_faro import make_predictions\n",
    "\n",
    "            sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "            for sentence, prediction in zip(sentences, predictions):\n",
    "\n",
    "                if prediction[1] in faro_classes:\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                    graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                    graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                    graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                    subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                    graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                    object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                    graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                    graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation betwee NERs\n",
    "                else:\n",
    "                    continue\n",
    "        else: # The data already contains the predictions\n",
    "            sentence = row['Sentence']\n",
    "            prediction = (row['Subject'], row['Relation'], row['Object'])\n",
    "\n",
    "            if prediction[1] in faro_classes:\n",
    "                sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((sentence_uri, nif.word, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((subject_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                graph.add((sentence_uri, nif.word, object_uri)) #Add the object to the sentence\n",
    "                graph.add((object_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation between NERs\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for event in ast.literal_eval(row['Event']): #Link the article to the corresponding event\n",
    "            #event_uri = node_creation('', event_name, base_add='/event')\n",
    "            graph.add((URIRef(row['URI']), schema.about, URIRef(event)))\n",
    "            graph.add((URIRef(event), schema.subjectOf, URIRef(row['URI'])))\n",
    "\n",
    "graph.serialize('Data/graphs/final_generated/eag_complete.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Used for drawing the graph\n",
    "import networkx as nx\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "\n",
    "nx_graph = rdflib_to_networkx_multidigraph(graph)\n",
    "pos = nx.spring_layout(nx_graph, scale=2)\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(nx_graph, 'r')\n",
    "nx.draw_networkx_edge_labels(nx_graph, pos, edge_labels=edge_labels)\n",
    "nx.draw(nx_graph, with_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge same entities\n",
    "### Entity coreference resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# This code will load the clusters from a text file, and merge the nodes in the cluster together\n",
    "import ast\n",
    "from resources import node_creation\n",
    "import pandas as pd\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "base_path = 'Data/cluster_data/output_all/'\n",
    "cluster_dirs = os.listdir(base_path)\n",
    "cluster_docs = [base_path + dir +'/event_clusters.txt' for dir in cluster_dirs if os.path.isdir(base_path + dir)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [03:39<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed mentions: 1863\n",
      "Number of double matches: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=Nc930a7b4451b4f59b3729ad19eb3628d (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph()\n",
    "graph.parse('Data/graphs/final_generated/eag_complete.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv') #Load the original dataset\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "failed_mentions = 0\n",
    "double_match = 0\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?mention\n",
    "WHERE {\n",
    "    ?mention a faro:Relata ;\n",
    "        rdf:value ?value\n",
    "}\"\"\"\n",
    "\n",
    "qres = graph.query(query, initNs={\"faro\": \"https://purl.org/faro/\", \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "all_mentions = [mention['mention'] for mention in qres.bindings]\n",
    "\n",
    "for doc in tqdm(cluster_docs):\n",
    "\n",
    "    with open(doc) as f: #Open the generated cluster file\n",
    "        cluster_doc = f.readlines()\n",
    "\n",
    "    for cluster in cluster_doc:\n",
    "\n",
    "        if cluster.startswith('['):\n",
    "            cluster = ast.literal_eval(cluster)\n",
    "\n",
    "            if len(cluster) != 1:\n",
    "                prev_mention_uri = None\n",
    "\n",
    "                for i, mention in enumerate(cluster):\n",
    "                    is_subject = False\n",
    "                    is_object = False\n",
    "                    mention_uri = None\n",
    "\n",
    "                    mention = mention.split('_')\n",
    "\n",
    "                    instance = data[(data['URI'] == mention[1]) & (data['Sentence_num'] == int(mention[2]))]\n",
    "                    sentence = instance['Sentence'].values[0]\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "\n",
    "                    #determine subject or object\n",
    "\n",
    "\n",
    "                    if any(part_mention in instance['Subject'].values[0] for part_mention in mention[0].split()):\n",
    "                        subject_mention_uri = node_creation('',instance['Subject'].values[0]  + str(sentence_uri), base_add='/subject')\n",
    "                        if subject_mention_uri in all_mentions: #Double check it\n",
    "                            is_subject = True\n",
    "\n",
    "\n",
    "                    if any(part_mention in instance['Object'].values[0] for part_mention in mention[0].split()):\n",
    "                        object_mention_uri = node_creation('',instance['Object'].values[0] + str(sentence_uri), base_add='/object')\n",
    "                        if object_mention_uri in all_mentions: #Double check again\n",
    "                            is_object = True\n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "                    if is_subject and is_object: #Look up in the graph, to see which is correct\n",
    "                        if subject_mention_uri in all_mentions:\n",
    "                            is_object = False\n",
    "                        elif object_mention_uri in all_mentions:\n",
    "                            is_subject = False\n",
    "                    \"\"\"\n",
    "\n",
    "                    if not (is_subject or is_object): #The mention could not be found\n",
    "                        #print(mention[0])\n",
    "                        failed_mentions +=1\n",
    "                        continue\n",
    "\n",
    "                    mention_uri = subject_mention_uri if is_subject else object_mention_uri\n",
    "\n",
    "                    if prev_mention_uri != None:\n",
    "                        if is_subject:\n",
    "                            graph.add((prev_mention_uri, owl.sameAs, subject_mention_uri))\n",
    "                        else:\n",
    "                            graph.add((prev_mention_uri, owl.sameAs, object_mention_uri))\n",
    "\n",
    "                    prev_mention_uri = mention_uri\n",
    "\n",
    "\n",
    "print(f\"Number of failed mentions: {failed_mentions}\\nNumber of double matches: {double_match}\")\n",
    "graph.serialize('Data/graphs/final_generated/eag_complete_merged.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select most relevant information from the graph\n",
    "### Lookup the values of the selected nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Graph, URIRef\n",
    "from resources import uri_validator\n",
    "from owlrl import DeductiveClosure\n",
    "\n",
    "namespaces = {\"faro\": \"https://purl.org/faro/\",\n",
    "              \"sem\": \"http://semanticweb.cs.vu.nl/2009/11/sem/\",\n",
    "              \"rnews\": \"http://iptc.org/std/rNews/2011-10-07#\",\n",
    "              \"nif\": \"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\",\n",
    "              \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "              \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "              \"schema\": \"http://schema.org/\"}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "graph.parse(\"Data/graphs/final_generated/eag_complete_merged.ttl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Lookup the values of the selected nodes --- Used when running the graph search model\n",
    "\n",
    "selected_nodes = pd.read_csv('Data/subgraph/5-subgraph.csv', index_col=0)\n",
    "graph = Graph()\n",
    "graph.parse(\"Data/graphs/event_article_graph_complete_merged.ttl\")\n",
    "\n",
    "subj_values = []\n",
    "obj_values = []\n",
    "\n",
    "subj_query = prepareQuery(\"\"\"\n",
    "    SELECT ?subj_value Where{\n",
    "\n",
    "    ?subject rdf:value ?subj_value.\n",
    "\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "\n",
    "obj_query = prepareQuery(\"\"\"\n",
    "    SELECT ?obj_value Where{\n",
    "\n",
    "    ?object rdf:value ?obj_value.\n",
    "\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "\n",
    "for index, row in selected_nodes.iterrows():\n",
    "\n",
    "\n",
    "    subj_node = URIRef(row['subject'])\n",
    "    obj_node = URIRef(row['object'])\n",
    "\n",
    "    subj_qres = graph.query(subj_query, initBindings={\"subject\": subj_node})\n",
    "    obj_qres = graph.query(obj_query, initBindings={\"object\": obj_node})\n",
    "\n",
    "    if len(subj_qres) != 0:\n",
    "        for row_result in subj_qres:\n",
    "\n",
    "            subj_values.append(row_result[0].value)\n",
    "            break\n",
    "\n",
    "    elif uri_validator(row['subject']) == False:\n",
    "        subj_values.append(row['subject'])\n",
    "\n",
    "    else:\n",
    "        subj_values.append(None)\n",
    "\n",
    "\n",
    "    if len(obj_qres) != 0:\n",
    "        for row_result in obj_qres:\n",
    "\n",
    "            obj_values.append(row_result[0].value)\n",
    "            break\n",
    "\n",
    "    elif uri_validator(row['object']) == False:\n",
    "        obj_values.append(row['object'])\n",
    "\n",
    "    else:\n",
    "        obj_values.append(None)\n",
    "\n",
    "selected_nodes['subject_values'] = subj_values\n",
    "selected_nodes['object_values'] = obj_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              subject  \\\n5           http://www.wikidata.org/entity/Q104045825   \n6           http://www.wikidata.org/entity/Q104045825   \n0   http://kflow.eurecom.fr/subject/9b1d7b1f-39e0-...   \n1   http://kflow.eurecom.fr/subject/29e8115d-fc8d-...   \n1   http://kflow.eurecom.fr/subject/cf5a69f1-0ff3-...   \n..                                                ...   \n0   http://kflow.eurecom.fr/subject/aaeca7f3-7b7e-...   \n1   http://kflow.eurecom.fr/subject/99868c3a-7b42-...   \n1   http://kflow.eurecom.fr/subject/0826e220-fc09-...   \n0   http://kflow.eurecom.fr/subject/9942c216-ec8c-...   \n1   http://kflow.eurecom.fr/subject/d613d251-6048-...   \n\n                                           predicate  \\\n5   http://semanticweb.cs.vu.nl/2009/11/sem/hasPlace   \n6    http://semanticweb.cs.vu.nl/2009/11/sem/hasTime   \n0                       https://purl.org/faro/causes   \n1                      https://purl.org/faro/enables   \n1                       https://purl.org/faro/causes   \n..                                               ...   \n0                      https://purl.org/faro/enables   \n1                       https://purl.org/faro/causes   \n1                       https://purl.org/faro/causes   \n0                       https://purl.org/faro/causes   \n1                       https://purl.org/faro/causes   \n\n                                               object   type_df  iteration  \\\n5   http://kflow.eurecom.fr/e5099279-c6f7-5391-bb8...  outgoing          1   \n6   http://kflow.eurecom.fr/ec3994e5-10f9-54d3-b89...  outgoing          1   \n0   http://kflow.eurecom.fr/object/c192afd0-b701-5...  outgoing          4   \n1   http://kflow.eurecom.fr/object/5e8f6cd0-6a9d-5...   ingoing          4   \n1   http://kflow.eurecom.fr/object/ffe64b81-5469-5...   ingoing          4   \n..                                                ...       ...        ...   \n0   http://kflow.eurecom.fr/object/02929a3c-0808-5...  outgoing          4   \n1   http://kflow.eurecom.fr/object/4006f33c-f4d8-5...   ingoing          4   \n1   http://kflow.eurecom.fr/object/4f5e7558-dca8-5...   ingoing          4   \n0   http://kflow.eurecom.fr/object/91b3f484-743a-5...  outgoing          4   \n1   http://kflow.eurecom.fr/object/6a77df3f-903a-5...   ingoing          4   \n\n                           subject_values              object_values  \n5   2021 Kyrgyz constitutional referendum                 Kyrgyzstan  \n6   2021 Kyrgyz constitutional referendum  2021-01-10 00:00:00+00:00  \n0                    coronavirus pandemic                  dependent  \n1                              referendum                     powers  \n1                                   ready                       work  \n..                                    ...                        ...  \n0                            dictatorship             Vladimir Putin  \n1                                  waited                  criticise  \n1                            reservations                  happiness  \n0                             vote-buying                     crisis  \n1                             cooperation                   telegram  \n\n[96 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>predicate</th>\n      <th>object</th>\n      <th>type_df</th>\n      <th>iteration</th>\n      <th>subject_values</th>\n      <th>object_values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>http://www.wikidata.org/entity/Q104045825</td>\n      <td>http://semanticweb.cs.vu.nl/2009/11/sem/hasPlace</td>\n      <td>http://kflow.eurecom.fr/e5099279-c6f7-5391-bb8...</td>\n      <td>outgoing</td>\n      <td>1</td>\n      <td>2021 Kyrgyz constitutional referendum</td>\n      <td>Kyrgyzstan</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>http://www.wikidata.org/entity/Q104045825</td>\n      <td>http://semanticweb.cs.vu.nl/2009/11/sem/hasTime</td>\n      <td>http://kflow.eurecom.fr/ec3994e5-10f9-54d3-b89...</td>\n      <td>outgoing</td>\n      <td>1</td>\n      <td>2021 Kyrgyz constitutional referendum</td>\n      <td>2021-01-10 00:00:00+00:00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>http://kflow.eurecom.fr/subject/9b1d7b1f-39e0-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/c192afd0-b701-5...</td>\n      <td>outgoing</td>\n      <td>4</td>\n      <td>coronavirus pandemic</td>\n      <td>dependent</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/29e8115d-fc8d-...</td>\n      <td>https://purl.org/faro/enables</td>\n      <td>http://kflow.eurecom.fr/object/5e8f6cd0-6a9d-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>referendum</td>\n      <td>powers</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/cf5a69f1-0ff3-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/ffe64b81-5469-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>ready</td>\n      <td>work</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>http://kflow.eurecom.fr/subject/aaeca7f3-7b7e-...</td>\n      <td>https://purl.org/faro/enables</td>\n      <td>http://kflow.eurecom.fr/object/02929a3c-0808-5...</td>\n      <td>outgoing</td>\n      <td>4</td>\n      <td>dictatorship</td>\n      <td>Vladimir Putin</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/99868c3a-7b42-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/4006f33c-f4d8-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>waited</td>\n      <td>criticise</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/0826e220-fc09-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/4f5e7558-dca8-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>reservations</td>\n      <td>happiness</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>http://kflow.eurecom.fr/subject/9942c216-ec8c-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/91b3f484-743a-5...</td>\n      <td>outgoing</td>\n      <td>4</td>\n      <td>vote-buying</td>\n      <td>crisis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/d613d251-6048-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/6a77df3f-903a-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>cooperation</td>\n      <td>telegram</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_nodes = selected_nodes[~selected_nodes['predicate'].isin(['http://schema.org/subjectOf', 'http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#word'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Query the graph: extract triples without using graph search algorithm\n",
    "## Extract: time, place, actor, contentLocation, contentReferenceTime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=Nea473e47acee4e24bd10b94dfbaa228c (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph()\n",
    "graph.parse(\"Data/graphs/final_generated/eag_complete_merged.ttl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "#Extract 4W's from a event\n",
    "from collections import defaultdict\n",
    "query = \"\"\"\n",
    "SELECT ?time ?place ?actor ?beginTime ?endTime ?timeStamp where {\n",
    "\t?event a  sem:Event.\n",
    "    OPTIONAL{?event sem:hasTime ?time}.\n",
    "    OPTIONAL{?event sem:hasPlace ?place}.\n",
    "    OPTIONAL{?event sem:hasActor ?actor}.\n",
    "    OPTIONAL{?event sem:hasBeginTimeStamp ?beginTime}\n",
    "    OPTIONAL{?event sem:hasEndTimeStamp ?endTime}\n",
    "    OPTIONAL{?event sem:hasTimeStamp ?timeStamp}\n",
    "}\"\"\"\n",
    "\n",
    "event = URIRef(\"http://www.wikidata.org/entity/Q102850603\")\n",
    "qres = graph.query(query, initNs= namespaces, initBindings={\"event\": event})\n",
    "four_W = {}\n",
    "for row in qres.bindings:\n",
    "    for key in row.keys():\n",
    "        if key not in four_W.keys():\n",
    "            four_W[key] = [row[key]]\n",
    "        else:\n",
    "            if row[key] not in four_W[key]:\n",
    "                four_W[key].append(row[key])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time - [rdflib.term.URIRef('http://kflow.eurecom.fr/123ae010-1d5c-5b52-91e2-e99b8d235f9f')]\n",
      "place - [rdflib.term.URIRef('http://kflow.eurecom.fr/1e1db619-bbb9-514e-a01d-41274a70dc01'), rdflib.term.URIRef('http://kflow.eurecom.fr/21015f6e-f163-5660-a883-2b011f761cb0')]\n",
      "actor - [rdflib.term.URIRef('http://kflow.eurecom.fr/e443dded-04d1-5412-ad0e-0cd83fb76e17')]\n",
      "beginTime - [rdflib.term.URIRef('http://kflow.eurecom.fr/8b115168-ada3-584a-8ac3-cae46b49b052')]\n",
      "endTime - [rdflib.term.URIRef('http://kflow.eurecom.fr/6e046c92-5193-53f4-876f-c22d2aa06e5f')]\n"
     ]
    }
   ],
   "source": [
    "for key, value in four_W.items():\n",
    "    print(f\"{key} - {value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from owlrl import OWLRL_Semantics #This is needed to allow owl reasoning over the sameAs links\n",
    "DeductiveClosure(OWLRL_Semantics).expand(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "#First, extract all the mentions from the sentences\n",
    "query = \"\"\"\n",
    "PREFIX faro: <https://purl.org/faro/>\n",
    "PREFIX sem: <http://semanticweb.cs.vu.nl/2009/11/sem/>\n",
    "PREFIX rnews: <http://iptc.org/std/rNews/2011-10-07#>\n",
    "PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "prefix schema: <http://schema.org/>\n",
    "select distinct ?mention where {\n",
    "\t?event a sem:Event;\n",
    "         schema:subjectOf ?article.\n",
    "    ?article nif:sentence ?sentence.\n",
    "    ?sentence nif:word ?mention.\n",
    "    ?mention owl:sameAs ?o .\n",
    "\n",
    "}\"\"\"\n",
    "event = URIRef(\"http://www.wikidata.org/entity/Q102850603\")\n",
    "qres = graph.query(query, initNs= namespaces, initBindings={\"event\": event})\n",
    "mentions = [mention[0] for mention in qres] #Save all mentions of this event"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "#Now, find the biggest cluster\n",
    "query = \"\"\"\n",
    "SELECT ?nodes where {\n",
    "\t?mention a faro:Relata;\n",
    "\t    owl:sameAs+ ?nodes\n",
    "}\n",
    "\"\"\"\n",
    "clusters = {} #the key will be one of the mentions in the set\n",
    "for mention in mentions:\n",
    "    mention = URIRef(mention)\n",
    "    qres = graph.query(query, initNs= namespaces, initBindings={\"mention\": mention})\n",
    "    mention_cluster = set()\n",
    "    for result in qres:\n",
    "        mention_cluster.add(result[0])\n",
    "    if not any(key in mention_cluster for key in clusters.keys()) and len(mention_cluster)!=1: #Only keep unique clusters, keep as key one of the elements in the cluster\n",
    "        clusters[mention] = mention_cluster\n",
    "clusters= dict(sorted(clusters.items(), key=lambda x:len(x[1]), reverse=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating the correct format for JointGT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16095 instances\n"
     ]
    }
   ],
   "source": [
    "from resources import gen_mapping_dict\n",
    "encoding_dict, total_instances = gen_mapping_dict('..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\val.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\test.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "from resources import MappingDict\n",
    "import pandas as pd\n",
    "\n",
    "def gen_jointgt_input_format(data, output_file, encoding_dict= None, subj_col= 'subject_values', rel_col= 'predicate', obj_col= 'object_values', sent_col = None, single_event = True, start_id = 0):\n",
    "    \"\"\"\n",
    "    This function converts a dataset to a format suitable for the jointGT model\n",
    "    :param data: The dataset to process\n",
    "    :param output_file: The name of the output file (.json should be added)\n",
    "    :param encoding_dict: The dictionary containing the encodings for the words\n",
    "    :param subj_col: The column that contains the subject\n",
    "    :param rel_col: The column that contains the relation\n",
    "    :param obj_col: The column that contains the object\n",
    "    :param sent_col: The column that contains the sentence\n",
    "    :param single_event: If the data is a single event, or each row should be seen as a single event\n",
    "    :param start_id: the instance ID from which the count should start\n",
    "    :return: the number of processed instances\n",
    "    \"\"\"\n",
    "\n",
    "    if encoding_dict == None:\n",
    "        encoding_dict = MappingDict()\n",
    "        #encoding_dict.add_words(data[subj_col].to_list()) #Generate unique ID's per word (subject)\n",
    "        encoding_dict.add_words(data[obj_col].to_list()) #Generate unique ID's per word (object)\n",
    "\n",
    "    else:\n",
    "        #encoding_dict.add_words(data[subj_col].to_list()) #For subject\n",
    "        encoding_dict.add_words(data[obj_col].to_list()) #For object\n",
    "\n",
    "    full_data = []\n",
    "    kbs = {}\n",
    "    data = data.reset_index(drop=True)\n",
    "    for index, row in data.iterrows():\n",
    "        #subj_id = encoding_dict.encoding_dict[row[subj_col]] #For subject\n",
    "        obj_id = encoding_dict.encoding_dict[row[obj_col]] #For object\n",
    "        relation = row[rel_col].split('/')[-1]\n",
    "        #kbs[subj_id] = [row[subj_col], row[subj_col], [[relation, str(row[obj_col])]]] #For subject\n",
    "        kbs[obj_id] = [row[obj_col], row[obj_col], [[relation, str(row[subj_col])]]]\n",
    "\n",
    "        if single_event == False:\n",
    "            json_dict = {\"id\": start_id +index,\n",
    "                         \"kbs\": kbs,\n",
    "                         \"text\": [row[sent_col]]}\n",
    "            full_data.append(json_dict)\n",
    "            kbs = {}\n",
    "\n",
    "\n",
    "    if single_event:\n",
    "        full_data = {\"id\": 1,\n",
    "                     \"kbs\": kbs,\n",
    "                     \"text\": [\"test\"]}\n",
    "\n",
    "    with open(output_file, \"w\") as json_out:\n",
    "\n",
    "        json.dump(full_data, json_out, indent = 2)\n",
    "\n",
    "    print(f\"Processed {len(data)} instances\")\n",
    "    return len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800 instances\n",
      "Processed 201 instances\n",
      "Processed 95 instances\n"
     ]
    }
   ],
   "source": [
    "#Now convert rebel relationship data to JointGT input format\n",
    "\n",
    "relation_data_train = pd.read_csv('Data/rebel_v2/data/new_split/train.csv', index_col=0)\n",
    "relation_data_val = pd.read_csv('Data/rebel_v2/data/new_split/val.csv', index_col=0)\n",
    "relation_data_test = pd.read_csv('Data/rebel_v2/data/new_split/test.csv', index_col=0)\n",
    "\n",
    "total_instances += gen_jointgt_input_format(relation_data_train, 'Data/jointGT/faro/relation_dataset_jointgt_train.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id=total_instances)\n",
    "total_instances += gen_jointgt_input_format(relation_data_val, 'Data/jointGT/faro/relation_dataset_jointgt_val.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id= total_instances)\n",
    "total_instances += gen_jointgt_input_format(relation_data_test, 'Data/jointGT/faro/relation_dataset_jointgt_test.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id= total_instances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_gen = json.load(open('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_train.json'))\n",
    "dataset_jointgt = json.load(open('..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Next step: Combine both the original WebNLG data with the newly generated one\n",
    "import random\n",
    "def combine_datasets(dataset1, dataset2, output_file):\n",
    "    dataset1 = json.load(open(dataset1))\n",
    "    dataset2 = json.load(open(dataset2))\n",
    "\n",
    "    combined = dataset1+dataset2\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    with open(output_file, \"w\") as json_out:\n",
    "        json.dump(combined, json_out, indent = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_train.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json', 'Data\\\\jointGT\\\\combined\\\\train.json')\n",
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_val.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\val.json', 'Data\\\\jointGT\\\\combined\\\\val.json')\n",
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_test.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\test.json', 'Data\\\\jointGT\\\\combined\\\\test.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}