{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate the event and article knowledge graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import ast\n",
    "from resources import *\n",
    "import re\n",
    "\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "nif = Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "faro = Namespace(\"https://purl.org/faro/\")\n",
    "sem = Namespace(\"http://semanticweb.cs.vu.nl/2009/11/sem/\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "\n",
    "faro_classes = {'cause': faro.causes, 'enable': faro.enables, 'intend': faro.intends_to_cause, 'prevent': faro.prevents} #dict of faro definitions\n",
    "sem_props = {'http://www.wikidata.org/prop/direct/P710': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P664': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P112': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P17': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P276': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P625': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P131': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P30': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P585': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P580': sem.hasBeginTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P582': sem.hasEndTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P571': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P576': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P577': sem.hasTimeStamp,\n",
    "             'http://www.w3.org/2000/01/rdf-schema#label': 'what'}\n",
    "\n",
    "sem_classes = {sem.hasActor: sem.Actor,\n",
    "               sem.hasPlace: sem.Place,\n",
    "               sem.hasTime: sem.Time,\n",
    "               sem.hasBeginTimeStamp: sem.Time,\n",
    "               sem.hasEndTimeStamp: sem.Time,\n",
    "               sem.hasTimeStamp: sem.Time,\n",
    "               'what': sem.Event}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from resources import clean_text\n",
    "data = pd.read_csv('Data/ASRAEL_data_full.csv')\n",
    "data['Text'] = data['Text'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when searching for event: http://www.wikidata.org/entity/Q100919128\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q113945893\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q105597606\n"
     ]
    }
   ],
   "source": [
    "#This converts the data into the right format, by removing uneccessary tokens in text and converting the wikidata link to text\n",
    "graph = Graph()\n",
    "\n",
    "event_mapping = {} #here the wikidata event urls and their names are saved\n",
    "failed_events= [] #Events that can't be found e.g. owl:sameAs need to be removed\n",
    "\n",
    "sparql = SPARQLWrapper(\n",
    "    \"https://query.wikidata.org/sparql\"\n",
    ")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "for event in data['Event'].unique().tolist():\n",
    "    event_ = f\"wd:{event.split('/')[-1]}\"\n",
    "\n",
    "    sparql.setQuery(\"\"\"\n",
    "    SELECT (?p as ?wiki_prop) (?o as ?result)\n",
    "    WHERE {{{\n",
    "\n",
    "        %s ?p ?temp.\n",
    "      ?temp rdfs:label ?o.\n",
    "      FILTER (lang(?o) = 'en') }\n",
    "      }\n",
    "\n",
    "      UNION\n",
    "\n",
    "      {\n",
    "       SELECT *\n",
    "       WHERE{\n",
    "        %s ?p ?o.\n",
    "         FILTER(lang(?o) = 'en' || lang(?o)='') }}\n",
    "     } \"\"\" % (event_, event_))\n",
    "\n",
    "    try:\n",
    "        result = sparql.queryAndConvert()\n",
    "        #event_name = ret['results']['bindings'][0]['item']['value']\n",
    "\n",
    "    except:\n",
    "        print(f\"Something went wrong when converting event: {event}\")\n",
    "\n",
    "    try:\n",
    "        event_data = pd.json_normalize(result[\"results\"][\"bindings\"])[['wiki_prop.value', 'result.value', 'result.datatype']]\n",
    "        event_data = event_data.rename(columns={\"wiki_prop.value\": \"property\", \"result.value\": \"value\", \"result.datatype\": \"datatype\"})\n",
    "        event_data = event_data.loc[event_data['property'].isin(sem_props.keys())].reset_index(drop=True) #Only keep the 4W attributes\n",
    "        event_data['property'] = event_data['property'].replace(sem_props)\n",
    "        event_name = event_data.loc[event_data['property'] == 'what']['value'].values[0] #This needs to be saved to map the wikidata urls to events\n",
    "        event_data = event_data[event_data.property != 'what'] #This row needs to be deleted for the loop\n",
    "        event_mapping[event] = event_name\n",
    "    except:\n",
    "        print(f\"Error when searching for event: {event}\")\n",
    "        failed_events.append(event)\n",
    "        continue\n",
    "\n",
    "    #event_uri = node_creation('', event_name, base_add='/event') #Generate the URI for the event\n",
    "    graph.add((URIRef(event), RDF.type, sem.Event)) #Create the event\n",
    "    graph.add((URIRef(event), RDF.value, Literal(event_name)))\n",
    "\n",
    "    for index, row in event_data.iterrows():\n",
    "        uri = node_creation('', row['value'], base_add='') #Generate the URI for the property\n",
    "        graph.add((uri, RDF.type, sem_classes[row['property']])) #Create the node for the property, and lookup its class\n",
    "        if pd.isna(row['datatype']) == False: #It has a declared datatype\n",
    "            graph.add((uri, RDF.value, Literal(row['value'], datatype=row['datatype'])))\n",
    "        else:\n",
    "            graph.add((uri, RDF.value, Literal(row['value']))) #Add the value of the relation to the graph\n",
    "        graph.add((URIRef(event), row['property'], uri)) #Connect the event to the property\n",
    "\n",
    "graph.serialize('Data/graphs/Event_graph_all.ttl', format='turtle')\n",
    "\n",
    "data= data[~data['Event'].isin(failed_events)] #remove the rows for which the event was not found\n",
    "#data['Event'] = data['Event'].map(event_mapping)\n",
    "#Check if this still is oke, it adds a list of events to the column event\n",
    "data = data.groupby(['URI','Identifier','Location', 'Time', 'Text']).agg({'Event': lambda x: list(x)}).reset_index(drop=False)\n",
    "data.to_csv('Data/ASRAEL_data_full_converted.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1122 [00:00<?, ?it/s]C:\\Users\\mike-\\Documents\\VU\\Eurecom\\KG_mapping\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1122 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform predictions again since the sentence number is needed for event resolution\n",
    "import pandas as pd\n",
    "from rebel_finetuning_faro import make_predictions\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('Data/ASRAEL_data_full_converted.csv')\n",
    "total_uri = []\n",
    "total_identifier = []\n",
    "total_location = []\n",
    "total_time = []\n",
    "total_event = []\n",
    "total_sentence_num = []\n",
    "total_sentences = []\n",
    "total_subject = []\n",
    "total_relation = []\n",
    "total_object = []\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "    for i, (sentence, prediction) in enumerate(zip(sentences, predictions)):\n",
    "\n",
    "        total_uri.append(row['URI'])\n",
    "        total_identifier.append(row['Identifier'])\n",
    "        total_location.append(row['Location'])\n",
    "        total_time.append(row['Time'])\n",
    "        total_event.append(row['Event'])\n",
    "        total_sentence_num.append(i)\n",
    "        total_sentences.append(sentence)\n",
    "        total_subject.append(prediction[0])\n",
    "        total_relation.append(prediction[1])\n",
    "        total_object.append(prediction[2])\n",
    "    break\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({'URI': total_uri, 'Identifier': total_identifier, 'Location': total_location, 'Time': total_time, 'Sentence_num': total_sentence_num, 'Sentence': total_sentences, 'Subject': total_subject, 'Relation': total_relation, 'Object': total_object, 'Event': total_event})\n",
    "\n",
    "new_data = new_data[~((new_data['Sentence'].str.len() < 25) & new_data['Sentence'].str.contains('/'))].reset_index(drop=True)\n",
    "\n",
    "new_data.to_csv('Data/final_data_with_predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19578/19578 [00:17<00:00, 1094.13it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/graphs/updated_ontology/eag_complete_new.ttl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py:791\u001B[0m, in \u001B[0;36mmove\u001B[1;34m(src, dst, copy_function)\u001B[0m\n\u001B[0;32m    790\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 791\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrename\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_dst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] Het systeem kan het opgegeven pad niet vinden: 'C:\\\\Users\\\\mike-\\\\AppData\\\\Local\\\\Temp\\\\tmp41fin0u_' -> 'Data/graphs/updated_ontology/eag_complete_new.ttl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 83\u001B[0m\n\u001B[0;32m     80\u001B[0m             graph\u001B[38;5;241m.\u001B[39madd((URIRef(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mURI\u001B[39m\u001B[38;5;124m'\u001B[39m]), schema\u001B[38;5;241m.\u001B[39mabout, URIRef(event)))\n\u001B[0;32m     81\u001B[0m             graph\u001B[38;5;241m.\u001B[39madd((URIRef(event), schema\u001B[38;5;241m.\u001B[39msubjectOf, URIRef(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mURI\u001B[39m\u001B[38;5;124m'\u001B[39m])))\n\u001B[1;32m---> 83\u001B[0m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserialize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mData/graphs/updated_ontology/eag_complete_new.ttl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mturtle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\VU\\Eurecom\\KG_mapping\\venv\\lib\\site-packages\\rdflib\\graph.py:1210\u001B[0m, in \u001B[0;36mGraph.serialize\u001B[1;34m(self, destination, format, base, encoding, **args)\u001B[0m\n\u001B[0;32m   1208\u001B[0m dest \u001B[38;5;241m=\u001B[39m url2pathname(path) \u001B[38;5;28;01mif\u001B[39;00m scheme \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m location\n\u001B[0;32m   1209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(shutil, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmove\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1210\u001B[0m     \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmove\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1212\u001B[0m     shutil\u001B[38;5;241m.\u001B[39mcopy(name, dest)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py:811\u001B[0m, in \u001B[0;36mmove\u001B[1;34m(src, dst, copy_function)\u001B[0m\n\u001B[0;32m    809\u001B[0m         rmtree(src)\n\u001B[0;32m    810\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 811\u001B[0m         \u001B[43mcopy_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_dst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m         os\u001B[38;5;241m.\u001B[39munlink(src)\n\u001B[0;32m    813\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m real_dst\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py:435\u001B[0m, in \u001B[0;36mcopy2\u001B[1;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(dst):\n\u001B[0;32m    434\u001B[0m     dst \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dst, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(src))\n\u001B[1;32m--> 435\u001B[0m \u001B[43mcopyfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_symlinks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_symlinks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    436\u001B[0m copystat(src, dst, follow_symlinks\u001B[38;5;241m=\u001B[39mfollow_symlinks)\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dst\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py:264\u001B[0m, in \u001B[0;36mcopyfile\u001B[1;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[0;32m    262\u001B[0m     os\u001B[38;5;241m.\u001B[39msymlink(os\u001B[38;5;241m.\u001B[39mreadlink(src), dst)\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 264\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(src, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fsrc, \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fdst:\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;66;03m# macOS\u001B[39;00m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m _HAS_FCOPYFILE:\n\u001B[0;32m    267\u001B[0m             \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Data/graphs/updated_ontology/eag_complete_new.ttl'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "\n",
    "graph = Graph()\n",
    "graph.parse('Data/graphs/Event_graph_all.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv')\n",
    "\n",
    "data_with_predictions = True #Set this to True if the cell above was executed\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\n",
    "    if not row.isnull().values.any(): #If nan is present skip the row\n",
    "\n",
    "        graph.add((URIRef(row['URI']), RDF.type, rnews.Article)) #Add the URI as article\n",
    "\n",
    "        identifier_uri = node_creation('', row['Identifier'], base_add='/identifier')\n",
    "        graph.add((URIRef(row['URI']), rnews.identifier, URIRef(identifier_uri))) #Link the PublicID to the article\n",
    "        graph.add((URIRef(identifier_uri), RDF.value, Literal(row['Identifier'])))\n",
    "\n",
    "        location_uri = node_creation('', row['Location'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentLocation, URIRef(location_uri)))\n",
    "        graph.add((URIRef(location_uri), RDF.value, Literal(row['Location'])))\n",
    "\n",
    "        time_uri = node_creation('', row['Time'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentReferenceTime, URIRef(time_uri)))\n",
    "        graph.add((URIRef(time_uri), RDF.value, Literal(row['Time'])))\n",
    "\n",
    "        if data_with_predictions == False: #Make the predictions\n",
    "\n",
    "            from rebel_finetuning_faro import make_predictions\n",
    "\n",
    "            sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "            for sentence, prediction in zip(sentences, predictions):\n",
    "\n",
    "                if prediction[1] in faro_classes:\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                    graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                    graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                    graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                    subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                    graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                    object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                    graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                    graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation betwee NERs\n",
    "                else:\n",
    "                    continue\n",
    "        else: # The data already contains the predictions\n",
    "            sentence = row['Sentence']\n",
    "            prediction = (row['Subject'], row['Relation'], row['Object'])\n",
    "\n",
    "            if prediction[1] in faro_classes:\n",
    "                sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((sentence_uri, nif.word, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((subject_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                graph.add((sentence_uri, nif.word, object_uri)) #Add the object to the sentence\n",
    "                graph.add((object_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation between NERs\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for event in ast.literal_eval(row['Event']): #Link the article to the corresponding event\n",
    "            #event_uri = node_creation('', event_name, base_add='/event')\n",
    "            graph.add((URIRef(row['URI']), schema.about, URIRef(event)))\n",
    "            graph.add((URIRef(event), schema.subjectOf, URIRef(row['URI'])))\n",
    "\n",
    "graph.serialize('Data/graphs/eag_complete_new.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Used for drawing the graph\n",
    "import networkx as nx\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "\n",
    "nx_graph = rdflib_to_networkx_multidigraph(graph)\n",
    "pos = nx.spring_layout(nx_graph, scale=2)\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(nx_graph, 'r')\n",
    "nx.draw_networkx_edge_labels(nx_graph, pos, edge_labels=edge_labels)\n",
    "nx.draw(nx_graph, with_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge same entities\n",
    "### Entity coreference resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# This code will load the clusters from a text file, and merge the nodes in the cluster together\n",
    "import ast\n",
    "from resources import node_creation\n",
    "import pandas as pd\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = 'Data/cluster_data/output_all/'\n",
    "cluster_dirs = os.listdir(base_path)\n",
    "cluster_docs = [base_path + dir +'/event_clusters.txt' for dir in cluster_dirs if os.path.isdir(base_path + dir)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [01:36<00:00,  1.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=N5f9c0946fd7342ba8dea1ce830fb0768 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph()\n",
    "graph.parse('Data/graphs/eag_complete_new.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv') #Load the original dataset\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "\n",
    "for doc in tqdm(cluster_docs):\n",
    "\n",
    "    with open(doc) as f: #Open the generated cluster file\n",
    "        cluster_doc = f.readlines()\n",
    "\n",
    "    for cluster in cluster_doc:\n",
    "\n",
    "        if cluster.startswith('['):\n",
    "            cluster = ast.literal_eval(cluster)\n",
    "\n",
    "            if len(cluster) != 1:\n",
    "                prev_mention_uri = None\n",
    "\n",
    "                for i, mention in enumerate(cluster):\n",
    "                    mention = mention.split('_')\n",
    "\n",
    "                    sentence = data[(data['URI'] == mention[1]) & (data['Sentence_num'] == int(mention[2]))]['Sentence'].values[0]\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "\n",
    "\n",
    "                    #determine subject or object\n",
    "                    if data[(data['URI'] == mention[1]) & (data['Sentence_num'] == int(mention[2]))]['Subject'].values[0] == mention[0]:\n",
    "                        mention_uri = node_creation('', mention[0] + str(sentence_uri), base_add='/subject')\n",
    "\n",
    "                    else:\n",
    "                        mention_uri = node_creation('', mention[0] + str(sentence_uri), base_add='/object')\n",
    "\n",
    "                    #print(mention_uri)\n",
    "\n",
    "                    if i != 0:\n",
    "                        graph.add((prev_mention_uri, owl.sameAs, mention_uri))\n",
    "\n",
    "                    prev_mention_uri = mention_uri\n",
    "graph.serialize('Data/graphs/eag_complete_new_merged.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select most relevant information from the graph\n",
    "### Lookup the values of the selected nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "from rdflib import Graph, URIRef\n",
    "from resources import uri_validator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Lookup the values of the selected nodes\n",
    "\n",
    "selected_nodes = pd.read_csv('Data/subgraph/5-subgraph.csv', index_col=0)\n",
    "graph = Graph()\n",
    "graph.parse(\"Data/graphs/event_article_graph_complete_merged.ttl\")\n",
    "\n",
    "subj_values = []\n",
    "obj_values = []\n",
    "\n",
    "subj_query = prepareQuery(\"\"\"\n",
    "    SELECT ?subj_value Where{\n",
    "\n",
    "    ?subject rdf:value ?subj_value.\n",
    "\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "\n",
    "obj_query = prepareQuery(\"\"\"\n",
    "    SELECT ?obj_value Where{\n",
    "\n",
    "    ?object rdf:value ?obj_value.\n",
    "\n",
    "    }\n",
    "\"\"\", initNs={\"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "\n",
    "for index, row in selected_nodes.iterrows():\n",
    "\n",
    "\n",
    "    subj_node = URIRef(row['subject'])\n",
    "    obj_node = URIRef(row['object'])\n",
    "\n",
    "    subj_qres = graph.query(subj_query, initBindings={\"subject\": subj_node})\n",
    "    obj_qres = graph.query(obj_query, initBindings={\"object\": obj_node})\n",
    "\n",
    "    if len(subj_qres) != 0:\n",
    "        for row_result in subj_qres:\n",
    "\n",
    "            subj_values.append(row_result[0].value)\n",
    "            break\n",
    "\n",
    "    elif uri_validator(row['subject']) == False:\n",
    "        subj_values.append(row['subject'])\n",
    "\n",
    "    else:\n",
    "        subj_values.append(None)\n",
    "\n",
    "\n",
    "    if len(obj_qres) != 0:\n",
    "        for row_result in obj_qres:\n",
    "\n",
    "            obj_values.append(row_result[0].value)\n",
    "            break\n",
    "\n",
    "    elif uri_validator(row['object']) == False:\n",
    "        obj_values.append(row['object'])\n",
    "\n",
    "    else:\n",
    "        obj_values.append(None)\n",
    "\n",
    "selected_nodes['subject_values'] = subj_values\n",
    "selected_nodes['object_values'] = obj_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              subject  \\\n5           http://www.wikidata.org/entity/Q104045825   \n6           http://www.wikidata.org/entity/Q104045825   \n0   http://kflow.eurecom.fr/subject/9b1d7b1f-39e0-...   \n1   http://kflow.eurecom.fr/subject/29e8115d-fc8d-...   \n1   http://kflow.eurecom.fr/subject/cf5a69f1-0ff3-...   \n..                                                ...   \n0   http://kflow.eurecom.fr/subject/aaeca7f3-7b7e-...   \n1   http://kflow.eurecom.fr/subject/99868c3a-7b42-...   \n1   http://kflow.eurecom.fr/subject/0826e220-fc09-...   \n0   http://kflow.eurecom.fr/subject/9942c216-ec8c-...   \n1   http://kflow.eurecom.fr/subject/d613d251-6048-...   \n\n                                           predicate  \\\n5   http://semanticweb.cs.vu.nl/2009/11/sem/hasPlace   \n6    http://semanticweb.cs.vu.nl/2009/11/sem/hasTime   \n0                       https://purl.org/faro/causes   \n1                      https://purl.org/faro/enables   \n1                       https://purl.org/faro/causes   \n..                                               ...   \n0                      https://purl.org/faro/enables   \n1                       https://purl.org/faro/causes   \n1                       https://purl.org/faro/causes   \n0                       https://purl.org/faro/causes   \n1                       https://purl.org/faro/causes   \n\n                                               object   type_df  iteration  \\\n5   http://kflow.eurecom.fr/e5099279-c6f7-5391-bb8...  outgoing          1   \n6   http://kflow.eurecom.fr/ec3994e5-10f9-54d3-b89...  outgoing          1   \n0   http://kflow.eurecom.fr/object/c192afd0-b701-5...  outgoing          4   \n1   http://kflow.eurecom.fr/object/5e8f6cd0-6a9d-5...   ingoing          4   \n1   http://kflow.eurecom.fr/object/ffe64b81-5469-5...   ingoing          4   \n..                                                ...       ...        ...   \n0   http://kflow.eurecom.fr/object/02929a3c-0808-5...  outgoing          4   \n1   http://kflow.eurecom.fr/object/4006f33c-f4d8-5...   ingoing          4   \n1   http://kflow.eurecom.fr/object/4f5e7558-dca8-5...   ingoing          4   \n0   http://kflow.eurecom.fr/object/91b3f484-743a-5...  outgoing          4   \n1   http://kflow.eurecom.fr/object/6a77df3f-903a-5...   ingoing          4   \n\n                           subject_values              object_values  \n5   2021 Kyrgyz constitutional referendum                 Kyrgyzstan  \n6   2021 Kyrgyz constitutional referendum  2021-01-10 00:00:00+00:00  \n0                    coronavirus pandemic                  dependent  \n1                              referendum                     powers  \n1                                   ready                       work  \n..                                    ...                        ...  \n0                            dictatorship             Vladimir Putin  \n1                                  waited                  criticise  \n1                            reservations                  happiness  \n0                             vote-buying                     crisis  \n1                             cooperation                   telegram  \n\n[96 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>predicate</th>\n      <th>object</th>\n      <th>type_df</th>\n      <th>iteration</th>\n      <th>subject_values</th>\n      <th>object_values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>http://www.wikidata.org/entity/Q104045825</td>\n      <td>http://semanticweb.cs.vu.nl/2009/11/sem/hasPlace</td>\n      <td>http://kflow.eurecom.fr/e5099279-c6f7-5391-bb8...</td>\n      <td>outgoing</td>\n      <td>1</td>\n      <td>2021 Kyrgyz constitutional referendum</td>\n      <td>Kyrgyzstan</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>http://www.wikidata.org/entity/Q104045825</td>\n      <td>http://semanticweb.cs.vu.nl/2009/11/sem/hasTime</td>\n      <td>http://kflow.eurecom.fr/ec3994e5-10f9-54d3-b89...</td>\n      <td>outgoing</td>\n      <td>1</td>\n      <td>2021 Kyrgyz constitutional referendum</td>\n      <td>2021-01-10 00:00:00+00:00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>http://kflow.eurecom.fr/subject/9b1d7b1f-39e0-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/c192afd0-b701-5...</td>\n      <td>outgoing</td>\n      <td>4</td>\n      <td>coronavirus pandemic</td>\n      <td>dependent</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/29e8115d-fc8d-...</td>\n      <td>https://purl.org/faro/enables</td>\n      <td>http://kflow.eurecom.fr/object/5e8f6cd0-6a9d-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>referendum</td>\n      <td>powers</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/cf5a69f1-0ff3-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/ffe64b81-5469-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>ready</td>\n      <td>work</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>http://kflow.eurecom.fr/subject/aaeca7f3-7b7e-...</td>\n      <td>https://purl.org/faro/enables</td>\n      <td>http://kflow.eurecom.fr/object/02929a3c-0808-5...</td>\n      <td>outgoing</td>\n      <td>4</td>\n      <td>dictatorship</td>\n      <td>Vladimir Putin</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/99868c3a-7b42-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/4006f33c-f4d8-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>waited</td>\n      <td>criticise</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/0826e220-fc09-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/4f5e7558-dca8-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>reservations</td>\n      <td>happiness</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>http://kflow.eurecom.fr/subject/9942c216-ec8c-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/91b3f484-743a-5...</td>\n      <td>outgoing</td>\n      <td>4</td>\n      <td>vote-buying</td>\n      <td>crisis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>http://kflow.eurecom.fr/subject/d613d251-6048-...</td>\n      <td>https://purl.org/faro/causes</td>\n      <td>http://kflow.eurecom.fr/object/6a77df3f-903a-5...</td>\n      <td>ingoing</td>\n      <td>4</td>\n      <td>cooperation</td>\n      <td>telegram</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_nodes = selected_nodes[~selected_nodes['predicate'].isin(['http://schema.org/subjectOf', 'http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#word'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating the correct format for JointGT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16095 instances\n"
     ]
    }
   ],
   "source": [
    "from resources import gen_mapping_dict\n",
    "encoding_dict, total_instances = gen_mapping_dict('..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\val.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\test.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "from resources import MappingDict\n",
    "import pandas as pd\n",
    "\n",
    "def gen_jointgt_input_format(data, output_file, encoding_dict= None, subj_col= 'subject_values', rel_col= 'predicate', obj_col= 'object_values', sent_col = None, single_event = True, start_id = 0):\n",
    "    \"\"\"\n",
    "    This function converts a dataset to a format suitable for the jointGT model\n",
    "    :param data: The dataset to process\n",
    "    :param output_file: The name of the output file (.json should be added)\n",
    "    :param encoding_dict: The dictionary containing the encodings for the words\n",
    "    :param subj_col: The column that contains the subject\n",
    "    :param rel_col: The column that contains the relation\n",
    "    :param obj_col: The column that contains the object\n",
    "    :param sent_col: The column that contains the sentence\n",
    "    :param single_event: If the data is a single event, or each row should be seen as a single event\n",
    "    :param start_id: the instance ID from which the count should start\n",
    "    :return: the number of processed instances\n",
    "    \"\"\"\n",
    "\n",
    "    if encoding_dict == None:\n",
    "        encoding_dict = MappingDict()\n",
    "        #encoding_dict.add_words(data[subj_col].to_list()) #Generate unique ID's per word (subject)\n",
    "        encoding_dict.add_words(data[obj_col].to_list()) #Generate unique ID's per word (object)\n",
    "\n",
    "    else:\n",
    "        #encoding_dict.add_words(data[subj_col].to_list()) #For subject\n",
    "        encoding_dict.add_words(data[obj_col].to_list()) #For object\n",
    "\n",
    "    full_data = []\n",
    "    kbs = {}\n",
    "    data = data.reset_index(drop=True)\n",
    "    for index, row in data.iterrows():\n",
    "        #subj_id = encoding_dict.encoding_dict[row[subj_col]] #For subject\n",
    "        obj_id = encoding_dict.encoding_dict[row[obj_col]] #For object\n",
    "        relation = row[rel_col].split('/')[-1]\n",
    "        #kbs[subj_id] = [row[subj_col], row[subj_col], [[relation, str(row[obj_col])]]] #For subject\n",
    "        kbs[obj_id] = [row[obj_col], row[obj_col], [[relation, str(row[subj_col])]]]\n",
    "\n",
    "        if single_event == False:\n",
    "            json_dict = {\"id\": start_id +index,\n",
    "                         \"kbs\": kbs,\n",
    "                         \"text\": [row[sent_col]]}\n",
    "            full_data.append(json_dict)\n",
    "            kbs = {}\n",
    "\n",
    "\n",
    "    if single_event:\n",
    "        full_data = {\"id\": 1,\n",
    "                     \"kbs\": kbs,\n",
    "                     \"text\": [\"test\"]}\n",
    "\n",
    "    with open(output_file, \"w\") as json_out:\n",
    "\n",
    "        json.dump(full_data, json_out, indent = 2)\n",
    "\n",
    "    print(f\"Processed {len(data)} instances\")\n",
    "    return len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800 instances\n",
      "Processed 201 instances\n",
      "Processed 95 instances\n"
     ]
    }
   ],
   "source": [
    "#Now convert rebel relationship data to JointGT input format\n",
    "\n",
    "relation_data_train = pd.read_csv('Data/rebel_v2/data/new_split/train.csv', index_col=0)\n",
    "relation_data_val = pd.read_csv('Data/rebel_v2/data/new_split/val.csv', index_col=0)\n",
    "relation_data_test = pd.read_csv('Data/rebel_v2/data/new_split/test.csv', index_col=0)\n",
    "\n",
    "total_instances += gen_jointgt_input_format(relation_data_train, 'Data/jointGT/faro/relation_dataset_jointgt_train.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id=total_instances)\n",
    "total_instances += gen_jointgt_input_format(relation_data_val, 'Data/jointGT/faro/relation_dataset_jointgt_val.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id= total_instances)\n",
    "total_instances += gen_jointgt_input_format(relation_data_test, 'Data/jointGT/faro/relation_dataset_jointgt_test.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id= total_instances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_gen = json.load(open('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_train.json'))\n",
    "dataset_jointgt = json.load(open('..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Next step: Combine both the original WebNLG data with the newly generated one\n",
    "import random\n",
    "def combine_datasets(dataset1, dataset2, output_file):\n",
    "    dataset1 = json.load(open(dataset1))\n",
    "    dataset2 = json.load(open(dataset2))\n",
    "\n",
    "    combined = dataset1+dataset2\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    with open(output_file, \"w\") as json_out:\n",
    "        json.dump(combined, json_out, indent = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_train.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json', 'Data\\\\jointGT\\\\combined\\\\train.json')\n",
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_val.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\val.json', 'Data\\\\jointGT\\\\combined\\\\val.json')\n",
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_test.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\test.json', 'Data\\\\jointGT\\\\combined\\\\test.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}