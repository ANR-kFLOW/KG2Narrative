{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate the event and article knowledge graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import ast\n",
    "from resources import *\n",
    "import re\n",
    "\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "nif = Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
    "faro = Namespace(\"https://purl.org/faro/\")\n",
    "sem = Namespace(\"http://semanticweb.cs.vu.nl/2009/11/sem/\")\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "rnews = Namespace(\"http://iptc.org/std/rNews/2011-10-07#\")\n",
    "schema = Namespace(\"http://schema.org/\")\n",
    "\n",
    "faro_classes = {'cause': faro.causes, 'enable': faro.enables, 'intend': faro.intends_to_cause, 'prevent': faro.prevents} #dict of faro definitions\n",
    "sem_props = {'http://www.wikidata.org/prop/direct/P710': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P664': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P112': sem.hasActor,\n",
    "             'http://www.wikidata.org/prop/direct/P17': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P276': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P625': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P131': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P30': sem.hasPlace,\n",
    "             'http://www.wikidata.org/prop/direct/P585': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P580': sem.hasBeginTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P582': sem.hasEndTimeStamp,\n",
    "             'http://www.wikidata.org/prop/direct/P571': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P576': sem.hasTime,\n",
    "             'http://www.wikidata.org/prop/direct/P577': sem.hasTimeStamp,\n",
    "             'http://www.w3.org/2000/01/rdf-schema#label': 'what'}\n",
    "\n",
    "sem_classes = {sem.hasActor: sem.Actor,\n",
    "               sem.hasPlace: sem.Place,\n",
    "               sem.hasTime: sem.Time,\n",
    "               sem.hasBeginTimeStamp: sem.Time,\n",
    "               sem.hasEndTimeStamp: sem.Time,\n",
    "               sem.hasTimeStamp: sem.Time,\n",
    "               'what': sem.Event}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from resources import clean_text\n",
    "data = pd.read_csv('Data/ASRAEL_data_full.csv')\n",
    "data['Text'] = data['Text'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when searching for event: http://www.wikidata.org/entity/Q100919128\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q113945893\n",
      "Error when searching for event: http://www.wikidata.org/entity/Q105597606\n"
     ]
    }
   ],
   "source": [
    "#This converts the data into the right format, by removing uneccessary tokens in text and converting the wikidata link to text\n",
    "from resources import uri_validator\n",
    "\n",
    "graph = Graph()\n",
    "place_set = set()\n",
    "time_set = set()\n",
    "actor_set = set()\n",
    "\n",
    "\n",
    "event_mapping = {} #here the wikidata event urls and their names are saved\n",
    "failed_events= [] #Events that can't be found e.g. owl:sameAs need to be removed\n",
    "\n",
    "sparql = SPARQLWrapper(\n",
    "    \"https://query.wikidata.org/sparql\"\n",
    ")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "for event in data['Event'].unique().tolist():\n",
    "    event_ = f\"wd:{event.split('/')[-1]}\"\n",
    "\n",
    "    sparql.setQuery(\"\"\"\n",
    "    SELECT (?p as ?wiki_prop) (?o as ?result)\n",
    "    WHERE {{{\n",
    "\n",
    "        %s ?p ?temp.\n",
    "      ?temp rdfs:label ?o.\n",
    "      FILTER (lang(?o) = 'en') }\n",
    "      }\n",
    "\n",
    "      UNION\n",
    "\n",
    "      {\n",
    "       SELECT *\n",
    "       WHERE{\n",
    "        %s ?p ?o.\n",
    "         FILTER(lang(?o) = 'en' || lang(?o)='') }}\n",
    "     } \"\"\" % (event_, event_))\n",
    "\n",
    "    try:\n",
    "        result = sparql.queryAndConvert()\n",
    "        #event_name = ret['results']['bindings'][0]['item']['value']\n",
    "\n",
    "    except:\n",
    "        print(f\"Something went wrong when converting event: {event}\")\n",
    "\n",
    "    try:\n",
    "        event_data = pd.json_normalize(result[\"results\"][\"bindings\"])[['wiki_prop.value', 'result.value', 'result.datatype']]\n",
    "        event_data = event_data.rename(columns={\"wiki_prop.value\": \"property\", \"result.value\": \"value\", \"result.datatype\": \"datatype\"})\n",
    "        event_data = event_data.loc[event_data['property'].isin(sem_props.keys())].reset_index(drop=True) #Only keep the 4W attributes\n",
    "        event_data['property'] = event_data['property'].replace(sem_props)\n",
    "        event_name = event_data.loc[event_data['property'] == 'what']['value'].values[0] #This needs to be saved to map the wikidata urls to events\n",
    "        event_data = event_data[event_data.property != 'what'] #This row needs to be deleted for the loop\n",
    "        event_mapping[event] = event_name\n",
    "    except:\n",
    "        print(f\"Error when searching for event: {event}\")\n",
    "        failed_events.append(event)\n",
    "        continue\n",
    "\n",
    "    #event_uri = node_creation('', event_name, base_add='/event') #Generate the URI for the event\n",
    "    graph.add((URIRef(event), RDF.type, sem.Event)) #Create the event\n",
    "    graph.add((URIRef(event), RDF.value, Literal(event_name)))\n",
    "\n",
    "    for index, row in event_data.iterrows():\n",
    "        uri = node_creation('', row['value'], base_add='') #Generate the URI for the property\n",
    "        if uri_validator(uri) == False:\n",
    "            print(f\"Found issue, generated link is not an uri:\\n{uri}\")\n",
    "        graph.add((uri, RDF.type, sem_classes[row['property']])) #Create the node for the property, and lookup its class\n",
    "        if pd.isna(row['datatype']) == False: #It has a declared datatype\n",
    "            graph.add((uri, RDF.value, Literal(row['value'], datatype=row['datatype'])))\n",
    "        else:\n",
    "            graph.add((uri, RDF.value, Literal(row['value']))) #Add the value of the relation to the graph\n",
    "        graph.add((URIRef(event), row['property'], uri)) #Connect the event to the property\n",
    "\n",
    "        if row['property'] == sem.hasActor:\n",
    "            actor_set.add(event)\n",
    "        elif row['property'] == sem.hasPlace:\n",
    "            place_set.add(event)\n",
    "        elif (row['property'] == sem.hasTime or row['property'] == sem.hasBeginTimeStamp or row['property'] == sem.hasEndTimeStamp ):\n",
    "            time_set.add(event)\n",
    "        else:\n",
    "            print(f\"property not supported: {row['property']}\")\n",
    "\n",
    "\n",
    "#graph.serialize('Data/graphs/final_generated/Event_graph_all.ttl', format='turtle')\n",
    "\n",
    "data= data[~data['Event'].isin(failed_events)] #remove the rows for which the event was not found\n",
    "#data['Event'] = data['Event'].map(event_mapping)\n",
    "#Check if this still is oke, it adds a list of events to the column event\n",
    "data = data.groupby(['URI','Identifier','Location', 'Time', 'Text']).agg({'Event': lambda x: list(x)}).reset_index(drop=False)\n",
    "#data.to_csv('Data/dataset_final_generated/ASRAEL_data_full_converted.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1122 [00:00<?, ?it/s]C:\\Users\\mike-\\Documents\\VU\\Eurecom\\KG_mapping\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1122 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform predictions, execute cell below to combine predictions and generate the graph at once.\n",
    "import pandas as pd\n",
    "from rebel_finetuning_faro import make_predictions\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('Data/dataset_final_generated/ASRAEL_data_full_converted.csv')\n",
    "total_uri = []\n",
    "total_identifier = []\n",
    "total_location = []\n",
    "total_time = []\n",
    "total_event = []\n",
    "total_sentence_num = []\n",
    "total_sentences = []\n",
    "total_subject = []\n",
    "total_relation = []\n",
    "total_object = []\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "    for i, (sentence, prediction) in enumerate(zip(sentences, predictions)):\n",
    "\n",
    "        total_uri.append(row['URI'])\n",
    "        total_identifier.append(row['Identifier'])\n",
    "        total_location.append(row['Location'])\n",
    "        total_time.append(row['Time'])\n",
    "        total_event.append(row['Event'])\n",
    "        total_sentence_num.append(i)\n",
    "        total_sentences.append(sentence)\n",
    "        total_subject.append(prediction[0])\n",
    "        total_relation.append(prediction[1])\n",
    "        total_object.append(prediction[2])\n",
    "    break\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({'URI': total_uri, 'Identifier': total_identifier, 'Location': total_location, 'Time': total_time, 'Sentence_num': total_sentence_num, 'Sentence': total_sentences, 'Subject': total_subject, 'Relation': total_relation, 'Object': total_object, 'Event': total_event})\n",
    "\n",
    "new_data = new_data[~((new_data['Sentence'].str.len() < 25) & new_data['Sentence'].str.contains('/'))].reset_index(drop=True)\n",
    "\n",
    "new_data.to_csv('Data/final_generated/final_data_with_predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19578/19578 [00:25<00:00, 754.16it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=N6eaa17f7cbe443be835e69bcc063e464 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "\n",
    "graph = Graph()\n",
    "graph.parse('Data/graphs/final_generated/Event_graph_all.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv')\n",
    "\n",
    "data_with_predictions = True #Set this to True if the cell above was executed\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\n",
    "    if not row.isnull().values.any(): #If nan is present skip the row\n",
    "\n",
    "        graph.add((URIRef(row['URI']), RDF.type, rnews.Article)) #Add the URI as article\n",
    "\n",
    "        identifier_uri = node_creation('', row['Identifier'], base_add='/identifier')\n",
    "        graph.add((URIRef(row['URI']), rnews.identifier, URIRef(identifier_uri))) #Link the PublicID to the article\n",
    "        graph.add((URIRef(identifier_uri), RDF.value, Literal(row['Identifier'])))\n",
    "\n",
    "        location_uri = node_creation('', row['Location'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentLocation, URIRef(location_uri)))\n",
    "        graph.add((URIRef(location_uri), RDF.value, Literal(row['Location'])))\n",
    "        graph.add((URIRef(location_uri), RDF.type, schema.Place))\n",
    "\n",
    "        time_uri = node_creation('', row['Time'], base_add='')\n",
    "        graph.add((URIRef(row['URI']), schema.contentReferenceTime, URIRef(time_uri)))\n",
    "        graph.add((URIRef(time_uri), RDF.value, Literal(row['Time'])))\n",
    "        graph.add((URIRef(time_uri), RDF.type, schema.Time))\n",
    "\n",
    "        if data_with_predictions == False: #Make the predictions\n",
    "\n",
    "            from rebel_finetuning_faro import make_predictions\n",
    "\n",
    "            sentences, predictions = make_predictions(tokenize.sent_tokenize(row['Text']), 'rebel_finetuned.pth')\n",
    "            for sentence, prediction in zip(sentences, predictions):\n",
    "\n",
    "                if prediction[1] in faro_classes:\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                    graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                    graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                    graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                    subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                    graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                    object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                    graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                    graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                    graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation betwee NERs\n",
    "                else:\n",
    "                    continue\n",
    "        else: # The data already contains the predictions\n",
    "            sentence = row['Sentence']\n",
    "            prediction = (row['Subject'], row['Relation'], row['Object'])\n",
    "\n",
    "            if prediction[1] in faro_classes:\n",
    "                sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "                graph.add((URIRef(row['URI']), nif.sentence, sentence_uri)) #Link the article to the sentence\n",
    "                graph.add((sentence_uri, RDF.type, nif.Sentence)) #Make the sentence URI of class 'Sentence'\n",
    "                graph.add((sentence_uri, RDF.value, Literal(sentence))) #Set the value of the URI equal to the sentence\n",
    "\n",
    "                subject_uri = node_creation('', prediction[0] + str(sentence_uri), base_add='/subject') #Generate the URI for the subject, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((sentence_uri, nif.word, subject_uri)) #Add the subject to the sentence\n",
    "                graph.add((subject_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((subject_uri, RDF.value, Literal(prediction[0]))) #Set the value of the subject URI equal to the subject\n",
    "\n",
    "                object_uri = node_creation('', prediction[2] + str(sentence_uri), base_add='/object') #Generate the URI for the object, for now add the uri of sentence to make it unique\n",
    "                #graph.add((sentence_uri, faro.Relata, object_uri)) #Add the object to the sentence\n",
    "                graph.add((sentence_uri, nif.word, object_uri)) #Add the object to the sentence\n",
    "                graph.add((object_uri, RDF.type, faro.Relata)) #Make it of class 'Relata'\n",
    "                graph.add((object_uri, RDF.value, Literal(prediction[2]))) #Set the value of the subject URI equal to the object\n",
    "\n",
    "                graph.add((subject_uri, faro_classes[prediction[1]], object_uri)) #Add relation between NERs\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for event in ast.literal_eval(row['Event']): #Link the article to the corresponding event\n",
    "            #event_uri = node_creation('', event_name, base_add='/event')\n",
    "            graph.add((URIRef(row['URI']), schema.about, URIRef(event)))\n",
    "            graph.add((URIRef(event), schema.subjectOf, URIRef(row['URI'])))\n",
    "\n",
    "graph.serialize('Data/graphs/final_generated/eag_complete.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Used for drawing the graph\n",
    "import networkx as nx\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_multidigraph\n",
    "\n",
    "nx_graph = rdflib_to_networkx_multidigraph(graph)\n",
    "pos = nx.spring_layout(nx_graph, scale=2)\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(nx_graph, 'r')\n",
    "nx.draw_networkx_edge_labels(nx_graph, pos, edge_labels=edge_labels)\n",
    "nx.draw(nx_graph, with_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge same entities\n",
    "### Entity coreference resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# This code will load the clusters from a text file, and merge the nodes in the cluster together\n",
    "import ast\n",
    "from resources import node_creation\n",
    "import pandas as pd\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "base_path = 'Data/cluster_data/output_all/'\n",
    "cluster_dirs = os.listdir(base_path)\n",
    "cluster_docs = [base_path + dir +'/event_clusters.txt' for dir in cluster_dirs if os.path.isdir(base_path + dir)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [03:39<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed mentions: 1863\n",
      "Number of double matches: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Graph identifier=Nc930a7b4451b4f59b3729ad19eb3628d (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph()\n",
    "graph.parse('Data/graphs/final_generated/eag_complete.ttl')\n",
    "data = pd.read_csv('Data/final_data_with_predictions.csv') #Load the original dataset\n",
    "owl = Namespace(\"http://www.w3.org/2002/07/owl#\")\n",
    "\n",
    "failed_mentions = 0\n",
    "double_match = 0\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?mention\n",
    "WHERE {\n",
    "    ?mention a faro:Relata ;\n",
    "        rdf:value ?value\n",
    "}\"\"\"\n",
    "\n",
    "qres = graph.query(query, initNs={\"faro\": \"https://purl.org/faro/\", \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"})\n",
    "all_mentions = [mention['mention'] for mention in qres.bindings]\n",
    "\n",
    "for doc in tqdm(cluster_docs):\n",
    "\n",
    "    with open(doc) as f: #Open the generated cluster file\n",
    "        cluster_doc = f.readlines()\n",
    "\n",
    "    for cluster in cluster_doc:\n",
    "\n",
    "        if cluster.startswith('['):\n",
    "            cluster = ast.literal_eval(cluster)\n",
    "\n",
    "            if len(cluster) != 1:\n",
    "                prev_mention_uri = None\n",
    "\n",
    "                for i, mention in enumerate(cluster):\n",
    "                    is_subject = False\n",
    "                    is_object = False\n",
    "                    mention_uri = None\n",
    "\n",
    "                    mention = mention.split('_')\n",
    "\n",
    "                    instance = data[(data['URI'] == mention[1]) & (data['Sentence_num'] == int(mention[2]))]\n",
    "                    sentence = instance['Sentence'].values[0]\n",
    "                    sentence_uri = node_creation('', sentence, base_add='/sentence') #Generate the URI for the sentence\n",
    "\n",
    "                    #determine subject or object\n",
    "\n",
    "\n",
    "                    if any(part_mention in instance['Subject'].values[0] for part_mention in mention[0].split()):\n",
    "                        subject_mention_uri = node_creation('',instance['Subject'].values[0]  + str(sentence_uri), base_add='/subject')\n",
    "                        if subject_mention_uri in all_mentions: #Double check it\n",
    "                            is_subject = True\n",
    "\n",
    "\n",
    "                    if any(part_mention in instance['Object'].values[0] for part_mention in mention[0].split()):\n",
    "                        object_mention_uri = node_creation('',instance['Object'].values[0] + str(sentence_uri), base_add='/object')\n",
    "                        if object_mention_uri in all_mentions: #Double check again\n",
    "                            is_object = True\n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "                    if is_subject and is_object: #Look up in the graph, to see which is correct\n",
    "                        if subject_mention_uri in all_mentions:\n",
    "                            is_object = False\n",
    "                        elif object_mention_uri in all_mentions:\n",
    "                            is_subject = False\n",
    "                    \"\"\"\n",
    "\n",
    "                    if not (is_subject or is_object): #The mention could not be found\n",
    "                        #print(mention[0])\n",
    "                        failed_mentions +=1\n",
    "                        continue\n",
    "\n",
    "                    mention_uri = subject_mention_uri if is_subject else object_mention_uri\n",
    "\n",
    "                    if prev_mention_uri != None:\n",
    "                        if is_subject:\n",
    "                            graph.add((prev_mention_uri, owl.sameAs, subject_mention_uri))\n",
    "                        else:\n",
    "                            graph.add((prev_mention_uri, owl.sameAs, object_mention_uri))\n",
    "\n",
    "                    prev_mention_uri = mention_uri\n",
    "\n",
    "\n",
    "print(f\"Number of failed mentions: {failed_mentions}\\nNumber of double matches: {double_match}\")\n",
    "graph.serialize('Data/graphs/final_generated/eag_complete_merged.ttl', format='turtle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select most relevant information from the graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Query the graph: extract triples without using graph search algorithm\n",
    "## Extract: time, place, actor, contentLocation, contentReferenceTime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N9b06690c3d16492aac5f0cc58fc80f98 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef\n",
    "from owlrl import DeductiveClosure\n",
    "from rdflib.term import Variable\n",
    "from owlrl import OWLRL_Semantics #This is needed to allow owl reasoning over the sameAs links\n",
    "import random\n",
    "\n",
    "namespaces = {\"faro\": \"https://purl.org/faro/\",\n",
    "              \"sem\": \"http://semanticweb.cs.vu.nl/2009/11/sem/\",\n",
    "              \"rnews\": \"http://iptc.org/std/rNews/2011-10-07#\",\n",
    "              \"nif\": \"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\",\n",
    "              \"owl\": \"http://www.w3.org/2002/07/owl#\",\n",
    "              \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "              \"schema\": \"http://schema.org/\"}\n",
    "\n",
    "graph = Graph()\n",
    "graph.parse(\"Data/graphs/final_generated/eag_complete_merged.ttl\")\n",
    "\n",
    "DeductiveClosure(OWLRL_Semantics).expand(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#Extract 4W's from a event\n",
    "event_uri = \"http://www.wikidata.org/entity/Q104705419\"\n",
    "query = \"\"\"\n",
    "SELECT ?event_name ?Time ?place ?actor ?beginTime ?endTime ?timeStamp where {\n",
    "\t?event a  sem:Event;\n",
    "\t    rdf:value ?event_name\n",
    "    OPTIONAL{?event sem:hasTime ?time_uri.\n",
    "            ?time_uri rdf:value ?Time}.\n",
    "    OPTIONAL{?event sem:hasPlace ?place_uri.\n",
    "            ?place_uri rdf:value ?place}.\n",
    "    OPTIONAL{?event sem:hasActor ?actor_uri.\n",
    "            ?actor_uri rdf:value ?actor}.\n",
    "    OPTIONAL{?event sem:hasBeginTimeStamp ?beginTime_uri.\n",
    "            ?beginTime_uri rdf:value ?beginTime}.\n",
    "    OPTIONAL{?event sem:hasEndTimeStamp ?endTime_uri.\n",
    "            ?endTime_uri rdf:value ?endTime}.\n",
    "    OPTIONAL{?event sem:hasTimeStamp ?time_uri.\n",
    "            ?time_uri rdf:value ?time}.\n",
    "}\"\"\"\n",
    "\n",
    "event = URIRef(event_uri)\n",
    "\n",
    "qres = graph.query(query, initNs= namespaces, initBindings={\"event\": event})\n",
    "four_W = {}\n",
    "\n",
    "for row in qres.bindings:\n",
    "    for key in row.keys():\n",
    "        if key not in four_W.keys():\n",
    "            four_W[key] = [row[key]]\n",
    "        else:\n",
    "            if row[key] not in four_W[key]:\n",
    "                four_W[key].append(row[key])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT (COUNT(?i) as ?num_input) WHERE {\n",
    "    ?i ?p2 ?uri\n",
    "} GROUP BY ?uri\n",
    "\"\"\"\n",
    "\n",
    "def def_value():\n",
    "    return 0\n",
    "\n",
    "four_W_scores = defaultdict(def_value)\n",
    "for values in four_W.values():\n",
    "    for value in values:\n",
    "        qres = graph.query(query, initNs= namespaces, initBindings={\"uri\": value})\n",
    "        four_W_scores[value] = int(qres.bindings[0]['num_input'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_name - [rdflib.term.Literal('2021 storming of the United States Capitol')]\n",
      "Time - [rdflib.term.Literal('2021-01-06T00:00:00+00:00', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#dateTime'))]\n",
      "place - [rdflib.term.Literal('Point(-77.009166666 38.889722222)', datatype=rdflib.term.URIRef('http://www.opengis.net/ont/geosparql#wktLiteral')), rdflib.term.Literal('United States of America'), rdflib.term.Literal('United States Capitol'), rdflib.term.Literal('Washington, D.C.')]\n",
      "actor - [rdflib.term.Literal('Proud Boys')]\n"
     ]
    }
   ],
   "source": [
    "for key, value in four_W.items():\n",
    "    print(f\"{key} - {value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "selected_nodes = {\"mentions\": []}\n",
    "\n",
    "if ('beginTime' in str(four_W.keys()) and 'endTime' in str(four_W.keys())):#Favour the more descriptive dates\n",
    "    if (four_W[Variable('beginTime')] != four_W[Variable('endTime')]): #If they are the same, it doesn't add any information\n",
    "        try:\n",
    "            del four_W[Variable('Time')]\n",
    "        except:\n",
    "            print(\"Key already removed\")\n",
    "\n",
    "    elif 'Time' in str(four_W.keys()):\n",
    "        del four_W[Variable('beginTime')]\n",
    "        del four_W[Variable('endTime')]\n",
    "\n",
    "\n",
    "for key in four_W:\n",
    "    most_relevant_node = None\n",
    "    for node in four_W[key]:\n",
    "        if four_W_scores[node] > four_W_scores[most_relevant_node]:\n",
    "            most_relevant_node = node\n",
    "    selected_nodes[str(key)] = str(most_relevant_node) if not 'Time' in str(key) else str(most_relevant_node).split('T')[0] #For now only take the date into account"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#First, extract all the mentions from the sentences\n",
    "query = \"\"\"\n",
    "PREFIX faro: <https://purl.org/faro/>\n",
    "PREFIX sem: <http://semanticweb.cs.vu.nl/2009/11/sem/>\n",
    "PREFIX rnews: <http://iptc.org/std/rNews/2011-10-07#>\n",
    "PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "prefix schema: <http://schema.org/>\n",
    "select distinct ?mention where {\n",
    "\t?event a sem:Event;\n",
    "         schema:subjectOf ?article.\n",
    "    ?article nif:sentence ?sentence.\n",
    "    ?sentence nif:word ?mention.\n",
    "    ?mention owl:sameAs ?o .\n",
    "\n",
    "}\"\"\"\n",
    "event = URIRef(event_uri)\n",
    "qres = graph.query(query, initNs= namespaces, initBindings={\"event\": event})\n",
    "mentions = [mention[0] for mention in qres] #Save all mentions of this event"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#Now, find the biggest cluster\n",
    "query = \"\"\"\n",
    "SELECT ?nodes where {\n",
    "\t?mention a faro:Relata;\n",
    "\t    owl:sameAs+ ?nodes\n",
    "}\n",
    "\"\"\"\n",
    "clusters = {} #the key will be one of the mentions in the set\n",
    "for mention in mentions:\n",
    "    mention = URIRef(mention)\n",
    "    qres = graph.query(query, initNs= namespaces, initBindings={\"mention\": mention})\n",
    "    mention_cluster = set()\n",
    "    for result in qres:\n",
    "        mention_cluster.add(result[0])\n",
    "    if not any(key in mention_cluster for key in clusters.keys()) and len(mention_cluster)!=1: #Only keep unique clusters, keep as key one of the elements in the cluster\n",
    "        clusters[mention] = mention_cluster\n",
    "clusters= dict(sorted(clusters.items(), key=lambda x:len(x[1]), reverse=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "#Find the text values for the clusters\n",
    "SELECT_MAX_MENTIONS = 3 #Select only the top x mentions\n",
    "\n",
    "subject_query = \"\"\"\n",
    "SELECT ?subject_value ?predicate ?object_value where {\n",
    "\t?mention a faro:Relata;\n",
    "\t    rdf:value ?subject_value;\n",
    "\t    ?predicate ?object.\n",
    "\t?object rdf:value ?object_value\n",
    "}\"\"\"\n",
    "\n",
    "object_query = \"\"\"\n",
    "SELECT ?subject_value ?predicate ?object_value where {\n",
    "\t?mention a faro:Relata;\n",
    "\t    rdf:value ?object_value.\n",
    "\t?subject ?predicate ?mention;\n",
    "\t    rdf:value ?subject_value\n",
    "}\"\"\"\n",
    "for i, cluster in enumerate(clusters.values()):\n",
    "    mention = random.choice(list(cluster)) #Only pick one mention per cluster\n",
    "    if 'subject' in str(mention):\n",
    "        qres = graph.query(subject_query, initNs= namespaces, initBindings={\"mention\": mention})\n",
    "    else:\n",
    "        qres = graph.query(object_query, initNs= namespaces, initBindings={\"mention\": mention})\n",
    "\n",
    "    for instance in qres.bindings:\n",
    "        if 'faro' in str(instance['predicate']):\n",
    "            selected_nodes['mentions'].append([str(instance['subject_value']), instance['predicate'].split('/')[-1], str(instance['object_value'])])\n",
    "            break\n",
    "    if i+1 == SELECT_MAX_MENTIONS:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "{'mentions': [['addressed', 'causes', 'stormed'],\n  ['died', 'causes', 'shot'],\n  ['measures', 'enables', 'removing']],\n 'event_name': '2021 storming of the United States Capitol',\n 'Time': '2021-01-06',\n 'place': 'United States of America',\n 'actor': 'Proud Boys'}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 instances\n"
     ]
    }
   ],
   "source": [
    "from resources import gen_mapping_dict\n",
    "mapping_dict, _ = gen_mapping_dict('Data/jointGT/events/events_combined.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating the correct format for JointGT\n",
    "## Convert the selected triples into the JointGT format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'resources' from 'C:\\\\Users\\\\mike-\\\\Documents\\\\VU\\\\Eurecom\\\\KG_mapping\\\\resources.py'>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, importlib\n",
    "importlib.reload(sys.modules['resources'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6 instances\n"
     ]
    }
   ],
   "source": [
    "from resources import convert_selected_triples_to_jointgt\n",
    "import os\n",
    "output_path = os.path.join(\"Data/jointGT/events/\", selected_nodes['event_name'].replace(\" \", \"_\"))\n",
    "convert_selected_triples_to_jointgt(selected_nodes, output_path + '.json', mapping_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Optional: merge different events together\n",
    "from resources import combine_jointgt_events\n",
    "import os\n",
    "\n",
    "filepaths = [os.path.join(\"Data/jointGT/events\", file) for file in os.listdir(\"Data/jointGT/events\")]\n",
    "combine_jointgt_events(filepaths, output_file= \"Data/jointGT/events/events_combined.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800 instances\n",
      "Processed 201 instances\n",
      "Processed 95 instances\n"
     ]
    }
   ],
   "source": [
    "#Now convert rebel relationship data to JointGT input format\n",
    "from resources import gen_jointgt_input_format, gen_mapping_dict\n",
    "\n",
    "encoding_dict, total_instances = gen_mapping_dict('..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\val.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\test.json')\n",
    "\n",
    "relation_data_train = pd.read_csv('Data/rebel_v2/data/new_split/train.csv', index_col=0)\n",
    "relation_data_val = pd.read_csv('Data/rebel_v2/data/new_split/val.csv', index_col=0)\n",
    "relation_data_test = pd.read_csv('Data/rebel_v2/data/new_split/test.csv', index_col=0)\n",
    "\n",
    "total_instances += gen_jointgt_input_format(relation_data_train, 'Data/jointGT/faro/relation_dataset_jointgt_train.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id=total_instances)\n",
    "total_instances += gen_jointgt_input_format(relation_data_val, 'Data/jointGT/faro/relation_dataset_jointgt_val.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id= total_instances)\n",
    "total_instances += gen_jointgt_input_format(relation_data_test, 'Data/jointGT/faro/relation_dataset_jointgt_test.json', encoding_dict, 'trigger1', 'label', 'trigger2', 'sentence', single_event= False, start_id= total_instances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Next step: Combine both the original WebNLG data with the newly generated one\n",
    "import random\n",
    "def combine_datasets(dataset1, dataset2, output_file):\n",
    "    dataset1 = json.load(open(dataset1))\n",
    "    dataset2 = json.load(open(dataset2))\n",
    "\n",
    "    combined = dataset1+dataset2\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    with open(output_file, \"w\") as json_out:\n",
    "        json.dump(combined, json_out, indent = 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_train.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\train.json', 'Data\\\\jointGT\\\\combined\\\\train.json')\n",
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_val.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\val.json', 'Data\\\\jointGT\\\\combined\\\\val.json')\n",
    "combine_datasets('Data\\\\jointGT\\\\faro\\\\relation_dataset_jointgt_test.json', '..\\\\JointGT\\\\JointGT_data\\\\data\\\\webnlg\\\\test.json', 'Data\\\\jointGT\\\\combined\\\\test.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32 instances\n"
     ]
    }
   ],
   "source": [
    "## Take the retrieved triples, and generate an input format for JointGT\n",
    "#For now just create the triples by hand\n",
    "#Changed 1 \"pandemic\" to \"Covid-19 pandemic\"\n",
    "import pandas as pd\n",
    "from resources import gen_jointgt_input_format_multiple\n",
    "\n",
    "#Note: The tuple (\"western sanctions\", \"prevent\", \"scientific and technological progress of Iran\"), (\"enemies' maximum pressure\", \"prevent\", \"scientific and technological progress of Iran\"), is not really prevent, just kept it in for showing the result on this relation.\n",
    "\n",
    "tuples_data = (\n",
    "            (\"Russia\", \"launch\", \"satellite\", \"None\", 0), (\"launch\", \"location\", \"Kazakhstan\", \"None\", 0), (\"launch\", \"time\", \"Tuesday\", \"None\", 0), (\"satellite\", \"enable\", \"military surveillance\", \"An Iranian satellite launched by Russia blasted off from Kazakhstan Tuesday and reached orbit amid controversy that Moscow might use it to boost its surveillance of military targets in Ukraine.\", 0),\n",
    "\n",
    "               (\"Russia\", \"cause\", \"invasion of Ukraine\", \"None\", 1), (\"invasion of Ukraine\", \"cause\", \"western sanctions\", \"None\", 1), (\"western sanctions\", \"cause\", \"Russia's international isolation\", \"None\", 1), (\"Russia\", \"intend\", \"find new clients\", \"As Russia's international isolation grows following Western sanctions over its invasion of Ukraine, the Kremlin is seeking to pivot towards the Middle East, Asia and Africa and find new clients for its embattled space programme.\", 1),\n",
    "\n",
    "               (\"Russian-Iranian bilateral cooperation\", \"enable\", \"implementation of new and even larger projects\", \"Speaking at the Moscow-controlled Baikonur Cosmodrome in the Kazakh steppe, Russian space chief Yury Borisov hailed 'an important milestone in Russian-Iranian bilateral cooperation, opening the way to the implementation of new and even larger projects'.\", 2),\n",
    "\n",
    "               (\"Iran Telecommunication\", \"minister\", \"Issa Zarepour\", \"None\", 3), (\"Issa Zarepour\", \"attended\", \"launch of the Khayyam satellite\", \"None\", 3), (\"satellite\", \"enable\", \"a turning point for the start of a new interaction\", \"Iran's Telecommunications Minister Issa Zarepour, who also attended the launch of the Khayyam satellite, called the event 'historic' and 'a turning point for the start of a new interaction in the field of space between our two countries'.\", 3),\n",
    "\n",
    "               (\"western sanctions\", \"not prevent\", \"scientific and technological progress of Iran\", \"None\", 4), (\"enemies' maximum pressure\", \"not prevent\", \"scientific and technological progress of Iran\", \"Nasser Kanani, the Iranian foreign ministry spokesman, said on Twitter that 'the brilliant path of scientific and technological progress of the Islamic republic of Iran continues despite sanctions and the enemies' maximum pressure'.\", 4),\n",
    "\n",
    "            (\"Iran\", \"maintained ties\", \"Moscow\", \"None\", 5), (\"Iran\", \"not criticized\", \"Ukraine invasion\", \"None\", 5), (\"Khayyam\", \"enable\", \"spy\", \"Iran, which has maintained ties with Moscow and refrained from criticism of the Ukraine invasion, has sought to deflect suspicions that Moscow could use Khayyam to spy on Ukraine.\", 5),\n",
    "\n",
    "            (\"Washington\", \"respond\", \"launch\", \"None\", 6), (\"Russia's growing cooperation with Iran\", \"cause\", \"profound threat\", \"Responding to the launch, Washington said Russia's growing cooperation with Iran should be viewed as a 'profound threat'.\", 6),\n",
    "\n",
    "\n",
    "               (\"satellite\", \"sends\", \"information\", \"None\", 7), (\"encrypted algorithm\", \"prevent\", \"third countries accessing information\", \"None\", 7), (\"satellite\", \"has\", \"encrypted algorithm\", \"'No third country is able to access the information' sent by the satellite due to its 'encrypted algorithm', it said.\", 7),\n",
    "\n",
    "               (\"Iran\", \"intend\", \"salvage 2015 deal\", \"None\", 8), (\"2015 deal\", \"prevent\", \"Iran's nuclear ambitions\", \"Iran is currently negotiating with world powers, including Moscow, to salvage a 2015 deal aimed at reining in Tehran's nuclear ambitions.\", 8),\n",
    "\n",
    "               (\"satellite\", \"contains\", \"ballistic missle technologies\", \"None\", 9), (\"ballistic missile technologies\", \"enable\", \"delivery of nuclear warhead\", \"Western governments worry that satellite launch systems incorporate technologies interchangeable with those used in ballistic missiles capable of delivering a nuclear warhead, something Iran has always denied wanting to build.\", 9),\n",
    "\n",
    "               (\"Iran\", \"launch\", \"first military satellite\", \"None\", 10), (\"first military satellite\", \"orbit\", \"April 2020\", \"None\", 10), (\"first military satellite\", \"cause\", \"sharp rebuke from the United States\", \"Iran successfully put its first military satellite into orbit in April 2020, drawing a sharp rebuke from the United States.\", 10),\n",
    "\n",
    "            (\"Borisov\", \"replaced\", \"Dmitry Rogozin\", \"None\", 11), (\"Borisov\", \"acknowledged\", \"difficult situation\", \"None\", 11), (\"tensions with the West\", \"cause\", \"difficult situation \", \"Borisov, who last month replaced bombastic nationalist Dmitry Rogozin as head of the Russian space agency, had acknowledged that the national space industry is in a 'difficult situation' amid tensions with the West.\", 11)\n",
    ")\n",
    "\n",
    "\n",
    "created_dataset = pd.DataFrame(tuples_data, columns =['subject_values', 'predicate', 'object_values', \"sentence\", 'instance'])\n",
    "\n",
    "total_instances = gen_jointgt_input_format_multiple(created_dataset, 'Data/jointGT/manual_generated_for_test/article4.json', sent_col=\"sentence\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 instances\n",
      "Processed 5 instances\n",
      "Processed 7 instances\n",
      "Processed 7 instances\n",
      "Processed 6 instances\n",
      "Processed 6 instances\n",
      "Processed 6 instances\n",
      "Processed 7 instances\n",
      "Processed 6 instances\n",
      "Processed 7 instances\n",
      "Processed 7 instances\n",
      "Processed 6 instances\n",
      "Processed 7 instances\n",
      "Done and saved\n",
      "Processed 13 events\n",
      "Saved at: test\\event_combined.json\n"
     ]
    }
   ],
   "source": [
    "#This generates the input format for the JointGT model from all the 4W events in the graph.\n",
    "#A list with events of interest can also be passed\n",
    "from event_selection_jointgt import bulk_generate\n",
    "\n",
    "bulk_generate(\"Data/graphs/final_generated/eag_complete_merged.ttl\", \"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}