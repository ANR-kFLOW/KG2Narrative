{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM #, BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.000025\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "SEED = 1\n",
    "SAVE_PATH = 'model.pth'\n",
    "data = pd.read_csv('drive/MyDrive/rebel_format_v2.csv')\n",
    "#df_train, df_val = train_test_split(data, test_size=0.1, random_state=SEED)\n",
    "df_train = pd.read_csv('train_rebel.csv')\n",
    "df_val = pd.read_csv('val_rebel.csv')\n",
    "del data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_checkpoint = \"Babelscape/rebel-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        txt = df['context'].tolist()\n",
    "        self.texts = tokenizer(txt, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        labels = df['triplets'].to_list()\n",
    "        self.labels = tokenizer(labels, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.texts.items()}\n",
    "        return item\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.labels.items()}\n",
    "        return item\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_data, batch_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_loop(model, df_train, df_val):\n",
    "    train_dataset = DataSequence(df_train)\n",
    "    val_dataset = DataSequence(df_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    # create a scheduler that reduces the learning rate by a factor of 0.1 every 10 epochs\n",
    "    #scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    best_metric = 0\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "\n",
    "            train_label = train_label['input_ids'].to(device)\n",
    "\n",
    "            mask = train_data['attention_mask'].to(device)\n",
    "            input_id = train_data['input_ids'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(input_id, mask, labels= train_label).loss\n",
    "            \n",
    "            total_loss_train += loss.item()\n",
    "        \n",
    "            loss.backward() # Update the weights\n",
    "            optimizer.step() # Notify optimizer that a batch is done.\n",
    "            optimizer.zero_grad() # Reset the optimer\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_loss_val = 0\n",
    "        pred = []\n",
    "        gt = []\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label['input_ids'].to(device)\n",
    "            mask = val_data['attention_mask'].to(device)\n",
    "            input_id = val_data['input_ids'].to(device)\n",
    "\n",
    "            loss = model(input_id, mask, labels=val_label).loss\n",
    "            total_loss_val += loss.item()\n",
    "            \n",
    "            outputs = model.generate(input_id)\n",
    "            outputs=tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "            labels = tokenizer.batch_decode(val_label, skip_special_tokens=False)\n",
    "\n",
    "            \n",
    "            gt = gt + extract_triplets(labels, gold_extraction=True)\n",
    "            pred = pred + extract_triplets(outputs, gold_extraction=False)\n",
    "\n",
    "            del outputs, labels\n",
    "        combined_metric = 0\n",
    "\n",
    "        scores, precision, recall, f1= re_score(pred, gt, 'relation')\n",
    "        combined_metric += scores[\"ALL\"][\"Macro_f1\"]\n",
    "\n",
    "        scores, precision, recall, f1= re_score(pred, gt, 'subject')\n",
    "        combined_metric += scores[\"ALL\"][\"Macro_f1\"]\n",
    "\n",
    "        scores, precision, recall, f1= re_score(pred, gt, 'object')\n",
    "        combined_metric = (combined_metric + scores[\"ALL\"][\"Macro_f1\"]) /3\n",
    "\n",
    "        best_metric = check_best_performing(model, best_metric, combined_metric, SAVE_PATH)\n",
    "        del scores, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "        # adjust the learning rate using the scheduler\n",
    "        #scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .6f} | Val_Loss: {total_loss_val / len(df_val): .6f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_triplets(texts, gold_extraction):\n",
    "    triplets = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            text = ''.join(text).replace('<s>', '').replace('</s>', '').replace('<pad>', '')\n",
    "            relation = ''\n",
    "            for token in text.split():\n",
    "                if token == \"<triplet>\":\n",
    "                    current = 't'\n",
    "                    if relation != '':\n",
    "                        triplets.append((subject, relation, object_))\n",
    "                        relation = ''\n",
    "                    subject = ''\n",
    "                elif token == \"<subj>\":\n",
    "                    current = 's'\n",
    "                    if relation != '':\n",
    "                        triplets.append((subject, relation, object_))\n",
    "                    object_ = ''\n",
    "                elif token == \"<obj>\":\n",
    "                    current = 'o'\n",
    "                    relation = ''\n",
    "                else:\n",
    "                    if current == 't':\n",
    "                        subject += ' ' + token\n",
    "                    elif current == 's':\n",
    "                        object_ += ' ' + token\n",
    "                    elif current == 'o':\n",
    "                        relation += ' ' + token\n",
    "            triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "        except:\n",
    "            if gold_extraction:\n",
    "                print(\"Gold labels should always be extracted correctly. Exiting\")\n",
    "                sys.exit()\n",
    "            triplets.append((\"Invalid\", \"Invalid\", \"Invalid\"))\n",
    "\n",
    "    return triplets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def re_score(predictions, ground_truths, type):\n",
    "    \"\"\"Evaluate RE predictions\n",
    "    Args:\n",
    "        predictions (list) :  list of list of predicted relations (several relations in each sentence)\n",
    "        ground_truths (list) :    list of list of ground truth relations\n",
    "            rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n",
    "                    \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n",
    "                    \"head_type\": ent_type,\n",
    "                    \"tail_type\": ent_type,\n",
    "                    \"type\": rel_type}\n",
    "        vocab (Vocab) :         dataset vocabulary\n",
    "        mode (str) :            in 'strict' or 'boundaries' \"\"\"\n",
    "    if type == 'relation':\n",
    "        vocab = ['cause', 'enable', 'prevent', 'intend']\n",
    "        predictions = [pred[1] for pred in predictions]\n",
    "        ground_truths = [gt[1] for gt in ground_truths]\n",
    "\n",
    "    elif type == 'subject':\n",
    "        predictions = [pred[0] for pred in predictions]\n",
    "        ground_truths = [gt[0] for gt in ground_truths]\n",
    "        #vocab = ['Invalid'] #Create the vocabulary of possible tags\n",
    "        vocab = np.unique(ground_truths).tolist()\n",
    "\n",
    "    elif type == 'object':\n",
    "        predictions = [pred[2] for pred in predictions]\n",
    "        ground_truths = [gt[2] for gt in ground_truths]\n",
    "        #vocab = ['Invalid']\n",
    "        vocab = np.unique(ground_truths).tolist()\n",
    "\n",
    "    scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in vocab + [\"ALL\"]}\n",
    "\n",
    "    # Count GT relations and Predicted relations\n",
    "    n_sents = len(ground_truths)\n",
    "    n_rels = n_sents #Since every 'sentence' has only 1 relation\n",
    "    n_found = n_sents\n",
    "\n",
    "    # Count TP, FP and FN per type\n",
    "    for pred_sent, gt_sent in zip(predictions, ground_truths):\n",
    "        for entity in vocab:\n",
    "\n",
    "            if pred_sent == entity:\n",
    "                pred_entities = {pred_sent}\n",
    "            else:\n",
    "                pred_entities = set()\n",
    "\n",
    "            if gt_sent == entity:\n",
    "                gt_entities = {gt_sent}\n",
    "\n",
    "            else:\n",
    "                gt_entities = set()\n",
    "\n",
    "            scores[entity][\"tp\"] += len(pred_entities & gt_entities)\n",
    "            scores[entity][\"fp\"] += len(pred_entities - gt_entities)\n",
    "            scores[entity][\"fn\"] += len(gt_entities - pred_entities)\n",
    "\n",
    "    # Compute per relation Precision / Recall / F1\n",
    "    for entity in scores.keys():\n",
    "        if scores[entity][\"tp\"]:\n",
    "            scores[entity][\"p\"] = 100 * scores[entity][\"tp\"] / (scores[entity][\"fp\"] + scores[entity][\"tp\"])\n",
    "            scores[entity][\"r\"] = 100 * scores[entity][\"tp\"] / (scores[entity][\"fn\"] + scores[entity][\"tp\"])\n",
    "        else:\n",
    "            scores[entity][\"p\"], scores[entity][\"r\"] = 0, 0\n",
    "\n",
    "        if not scores[entity][\"p\"] + scores[entity][\"r\"] == 0:\n",
    "            scores[entity][\"f1\"] = 2 * scores[entity][\"p\"] * scores[entity][\"r\"] / (\n",
    "                    scores[entity][\"p\"] + scores[entity][\"r\"])\n",
    "        else:\n",
    "            scores[entity][\"f1\"] = 0\n",
    "\n",
    "    # Compute micro F1 Scores\n",
    "    tp = sum([scores[entity][\"tp\"] for entity in vocab])\n",
    "    fp = sum([scores[entity][\"fp\"] for entity in vocab])\n",
    "    fn = sum([scores[entity][\"fn\"] for entity in vocab])\n",
    "\n",
    "    if tp:\n",
    "        precision = 100 * tp / (tp + fp)\n",
    "        recall = 100 * tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    else:\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "    scores[\"ALL\"][\"p\"] = precision\n",
    "    scores[\"ALL\"][\"r\"] = recall\n",
    "    scores[\"ALL\"][\"f1\"] = f1\n",
    "    scores[\"ALL\"][\"tp\"] = tp\n",
    "    scores[\"ALL\"][\"fp\"] = fp\n",
    "    scores[\"ALL\"][\"fn\"] = fn\n",
    "\n",
    "    # Compute Macro F1 Scores\n",
    "    scores[\"ALL\"][\"Macro_f1\"] = np.mean([scores[ent_type][\"f1\"] for ent_type in vocab])\n",
    "    scores[\"ALL\"][\"Macro_p\"] = np.mean([scores[ent_type][\"p\"] for ent_type in vocab])\n",
    "    scores[\"ALL\"][\"Macro_r\"] = np.mean([scores[ent_type][\"r\"] for ent_type in vocab])\n",
    "\n",
    "    #print(f\"RE Evaluation in *** {mode.upper()} *** mode\")\n",
    "\n",
    "    if type == 'relation':\n",
    "        print(\n",
    "            \"processed {} sentences with {} entities; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,\n",
    "                                                                                                 tp))\n",
    "        print(\n",
    "            \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n",
    "                scores[\"ALL\"][\"tp\"],\n",
    "                scores[\"ALL\"][\"fp\"],\n",
    "                scores[\"ALL\"][\"fn\"]))\n",
    "        print(\n",
    "            \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n",
    "                precision,\n",
    "                recall,\n",
    "                f1))\n",
    "        print(\n",
    "            \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n",
    "                scores[\"ALL\"][\"Macro_p\"],\n",
    "                scores[\"ALL\"][\"Macro_r\"],\n",
    "                scores[\"ALL\"][\"Macro_f1\"]))\n",
    "\n",
    "        for entity in vocab:\n",
    "            print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n",
    "                entity,\n",
    "                scores[entity][\"tp\"],\n",
    "                scores[entity][\"fp\"],\n",
    "                scores[entity][\"fn\"],\n",
    "                scores[entity][\"p\"],\n",
    "                scores[entity][\"r\"],\n",
    "                scores[entity][\"f1\"],\n",
    "                scores[entity][\"tp\"] +\n",
    "                scores[entity][\n",
    "                    \"fp\"]))\n",
    "\n",
    "    else:\n",
    "        print(f\"Macro F1 for {type}: {scores['ALL']['Macro_f1']:.4f}\")\n",
    "\n",
    "    return scores, precision, recall, f1\n",
    "\n",
    "\n",
    "def calc_acc(predictions, gold):\n",
    "    num_ner = len(predictions) #The total number of entities\n",
    "    acc_subj_correct = 0\n",
    "    acc_obj_correct = 0\n",
    "\n",
    "    for pred, gt in zip(predictions, gold):\n",
    "        if pred[0] == gt[0]: #The subjects match\n",
    "            acc_subj_correct += 1\n",
    "        \n",
    "        if pred[2] == gt[2]: #The objects match\n",
    "            acc_obj_correct +=1\n",
    "    \n",
    "    acc_subj_correct = acc_subj_correct / num_ner\n",
    "    acc_obj_correct = acc_obj_correct / num_ner\n",
    "\n",
    "    print(f\"acc subject: {acc_subj_correct} acc object: {acc_obj_correct}\")\n",
    "\n",
    "    return acc_subj_correct, acc_obj_correct\n",
    "\n",
    "\n",
    "def check_best_performing(model, best_metric, new_metric, PATH):\n",
    "    if new_metric > best_metric:\n",
    "        torch.save(model, PATH)\n",
    "        print(\"New best model found, saving...\")\n",
    "        best_metric = new_metric\n",
    "    return best_metric\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loop(model, df_train, df_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_model(data, model):\n",
    "    test_dataset = DataSequence(data)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=4)\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    gt = []\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    for val_data, val_label in test_dataloader:\n",
    "\n",
    "        test_label = val_label['input_ids'].to(device)\n",
    "        mask = val_data['attention_mask'].to(device)\n",
    "        input_id = val_data['input_ids'].to(device)\n",
    "\n",
    "\n",
    "        outputs = model.generate(input_id)\n",
    "        outputs=tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        labels = tokenizer.batch_decode(test_label, skip_special_tokens=False)\n",
    "\n",
    "        gt = gt + extract_triplets(labels, gold_extraction=True)\n",
    "        pred = pred + extract_triplets(outputs, gold_extraction=False)\n",
    "\n",
    "        del outputs, labels\n",
    "\n",
    "    scores, precision, recall, f1= re_score(pred, gt, 'relation')\n",
    "    scores, precision, recall, f1= re_score(pred, gt, 'subject')\n",
    "    scores, precision, recall, f1= re_score(pred, gt, 'object')\n",
    "\n",
    "test_data = pd.read_csv('test_rebel.csv')\n",
    "model = torch.load('drive/MyDrive/rebel_finetuned.pth')\n",
    "tokenizer = AutoTokenizer.from_pretrained('Babelscape/rebel-large')\n",
    "test_model(test_data, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''''\n",
    "texts = pd.read_csv('drive/MyDrive/rebel_format_v2.csv').head(100)\n",
    "texts = texts['context'].tolist()\n",
    "predictions = []\n",
    "\n",
    "model = torch.load('drive/MyDrive/rebel_model_finetuned.pth').to('cuda:0')\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('Babelscape/rebel-large')\n",
    "\n",
    "\n",
    "for sentence in texts:\n",
    "\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda:0\")\n",
    "    outputs = model.generate(**encoding, do_sample=True)\n",
    "    outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "    predictions += extract_triplets(outputs, gold_extraction=False)\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns=['subject', 'relation', 'object'])\n",
    "predictions['sentence']= texts\n",
    "predictions.to_csv('predictions.csv')\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}