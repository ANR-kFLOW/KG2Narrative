06/21/2023 15:15:27 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=True, checkpoint=None, clean_up_spaces=False, dataset='webnlg', debug=False, do_lowercase=False, do_predict=False, do_pretrain=False, do_train=True, eval_period=800, gradient_accumulation_steps=1, knowledge_file='knowledge-full.json', learning_rate=5e-05, length_penalty=1.0, mask_prob='[0.4,0.2,0.2,0.4,0.2]', max_edge_length=60, max_grad_norm=1.0, max_input_length=256, max_node_length=50, max_output_length=128, model_name='t5', model_path='JointGT_data/pretrain_model/jointgt_t5', no_lr_decay=False, num_beams=5, num_train_epochs=30.0, num_workers=1, output_dir='../../../data/dekok/out_normal/jointgt_t5_webnlg_normal', predict_batch_size=4, predict_file='JointGT_data/data/webnlg/val', prefix='', remove_bos=False, save_period=1000, seed=42, task_ratio='[0.4,0.4,0.2]', tokenizer_path='JointGT_data/pretrain_model/jointgt_t5', train_batch_size=4, train_file='JointGT_data/data/webnlg/train', wait_step=10, warmup_proportion=0.01, warmup_steps=1600, weight_decay=0.0)
06/21/2023 15:15:27 - INFO - __main__ - ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal
06/21/2023 15:15:27 - INFO - __main__ - Using 1 gpus
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - Model name 'JointGT_data/pretrain_model/jointgt_t5' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming 'JointGT_data/pretrain_model/jointgt_t5' is a path, a model identifier, or url to a directory containing tokenizer files.
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - Didn't find file JointGT_data/pretrain_model/jointgt_t5/added_tokens.json. We won't load it.
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - Didn't find file JointGT_data/pretrain_model/jointgt_t5/special_tokens_map.json. We won't load it.
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - Didn't find file JointGT_data/pretrain_model/jointgt_t5/tokenizer_config.json. We won't load it.
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - loading file JointGT_data/pretrain_model/jointgt_t5/spiece.model
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - loading file None
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - loading file None
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - loading file None
06/21/2023 15:15:27 - INFO - transformers.tokenization_utils_base - loading file JointGT_data/pretrain_model/jointgt_t5/tokenizer.json
06/21/2023 15:15:28 - INFO - transformers.configuration_utils - loading configuration file JointGT_data/pretrain_model/jointgt_t5/config.json
06/21/2023 15:15:28 - INFO - transformers.configuration_utils - Model config T5Config {
  "architectures": [
    "MyT5Pretrain"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "vocab_size": 32128
}

06/21/2023 15:15:28 - INFO - transformers.modeling_utils - loading weights file JointGT_data/pretrain_model/jointgt_t5/pytorch_model.bin
06/21/2023 15:15:37 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing MyT5ForConditionalGeneration.

06/21/2023 15:15:37 - INFO - transformers.modeling_utils - All the weights of MyT5ForConditionalGeneration were initialized from the model checkpoint at JointGT_data/pretrain_model/jointgt_t5.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyT5ForConditionalGeneration for predictions without further training.
06/21/2023 15:15:40 - INFO - __main__ - Starting training!
06/21/2023 15:23:46 - INFO - __main__ - Step 800 Train loss 2.01 Learning rate 2.50e-05 BLEU 39.10% on epoch=0
06/21/2023 15:23:46 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 15:23:47 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 15:23:47 - INFO - __main__ - Saving model with best BLEU: -100.00% -> 39.10% on epoch=0, global_step=800
06/21/2023 15:32:38 - INFO - __main__ - Step 1600 Train loss 1.03 Learning rate 5.00e-05 BLEU 49.15% on epoch=0
06/21/2023 15:32:38 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 15:32:44 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 15:32:44 - INFO - __main__ - Saving model with best BLEU: 39.10% -> 49.15% on epoch=0, global_step=1600
06/21/2023 15:41:29 - INFO - __main__ - Step 2400 Train loss 0.89 Learning rate 4.96e-05 BLEU 54.71% on epoch=0
06/21/2023 15:41:29 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 15:41:35 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 15:41:35 - INFO - __main__ - Saving model with best BLEU: 49.15% -> 54.71% on epoch=0, global_step=2400
06/21/2023 15:50:46 - INFO - __main__ - Step 3200 Train loss 0.83 Learning rate 4.92e-05 BLEU 57.50% on epoch=0
06/21/2023 15:50:46 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 15:50:52 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 15:50:52 - INFO - __main__ - Saving model with best BLEU: 54.71% -> 57.50% on epoch=0, global_step=3200
06/21/2023 16:00:01 - INFO - __main__ - Step 4000 Train loss 0.76 Learning rate 4.87e-05 BLEU 60.94% on epoch=1
06/21/2023 16:00:01 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 16:00:07 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 16:00:07 - INFO - __main__ - Saving model with best BLEU: 57.50% -> 60.94% on epoch=1, global_step=4000
06/21/2023 16:09:21 - INFO - __main__ - Step 4800 Train loss 0.74 Learning rate 4.83e-05 BLEU 60.37% on epoch=1
06/21/2023 16:18:31 - INFO - __main__ - Step 5600 Train loss 0.72 Learning rate 4.79e-05 BLEU 61.14% on epoch=1
06/21/2023 16:18:31 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 16:18:38 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 16:18:38 - INFO - __main__ - Saving model with best BLEU: 60.94% -> 61.14% on epoch=1, global_step=5600
06/21/2023 16:28:03 - INFO - __main__ - Step 6400 Train loss 0.71 Learning rate 4.75e-05 BLEU 62.33% on epoch=1
06/21/2023 16:28:03 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 16:28:10 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 16:28:10 - INFO - __main__ - Saving model with best BLEU: 61.14% -> 62.33% on epoch=1, global_step=6400
06/21/2023 16:37:22 - INFO - __main__ - Step 7200 Train loss 0.67 Learning rate 4.71e-05 BLEU 62.18% on epoch=2
06/21/2023 16:46:36 - INFO - __main__ - Step 8000 Train loss 0.68 Learning rate 4.66e-05 BLEU 62.01% on epoch=2
06/21/2023 16:55:42 - INFO - __main__ - Step 8800 Train loss 0.66 Learning rate 4.62e-05 BLEU 62.77% on epoch=2
06/21/2023 16:55:42 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 16:55:49 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 16:55:49 - INFO - __main__ - Saving model with best BLEU: 62.33% -> 62.77% on epoch=2, global_step=8800
06/21/2023 17:05:50 - INFO - __main__ - Step 9600 Train loss 0.66 Learning rate 4.58e-05 BLEU 63.39% on epoch=2
06/21/2023 17:05:50 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 17:05:57 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 17:05:57 - INFO - __main__ - Saving model with best BLEU: 62.77% -> 63.39% on epoch=2, global_step=9600
06/21/2023 17:16:23 - INFO - __main__ - Step 10400 Train loss 0.63 Learning rate 4.54e-05 BLEU 64.20% on epoch=3
06/21/2023 17:16:23 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 17:16:30 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 17:16:30 - INFO - __main__ - Saving model with best BLEU: 63.39% -> 64.20% on epoch=3, global_step=10400
06/21/2023 17:26:26 - INFO - __main__ - Step 11200 Train loss 0.62 Learning rate 4.49e-05 BLEU 63.67% on epoch=3
06/21/2023 17:35:37 - INFO - __main__ - Step 12000 Train loss 0.62 Learning rate 4.45e-05 BLEU 63.64% on epoch=3
06/21/2023 17:44:56 - INFO - __main__ - Step 12800 Train loss 0.61 Learning rate 4.41e-05 BLEU 64.33% on epoch=3
06/21/2023 17:44:56 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 17:45:03 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 17:45:03 - INFO - __main__ - Saving model with best BLEU: 64.20% -> 64.33% on epoch=3, global_step=12800
06/21/2023 17:55:13 - INFO - __main__ - Step 13600 Train loss 0.61 Learning rate 4.37e-05 BLEU 64.60% on epoch=4
06/21/2023 17:55:13 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 17:55:25 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 17:55:25 - INFO - __main__ - Saving model with best BLEU: 64.33% -> 64.60% on epoch=4, global_step=13600
06/21/2023 18:05:52 - INFO - __main__ - Step 14400 Train loss 0.60 Learning rate 4.33e-05 BLEU 64.60% on epoch=4
06/21/2023 18:14:59 - INFO - __main__ - Step 15200 Train loss 0.59 Learning rate 4.28e-05 BLEU 64.31% on epoch=4
06/21/2023 18:24:41 - INFO - __main__ - Step 16000 Train loss 0.59 Learning rate 4.24e-05 BLEU 65.55% on epoch=4
06/21/2023 18:24:41 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 18:24:50 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 18:24:50 - INFO - __main__ - Saving model with best BLEU: 64.60% -> 65.55% on epoch=4, global_step=16000
06/21/2023 18:34:07 - INFO - __main__ - Step 16800 Train loss 0.57 Learning rate 4.20e-05 BLEU 66.39% on epoch=5
06/21/2023 18:34:07 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 18:34:15 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 18:34:15 - INFO - __main__ - Saving model with best BLEU: 65.55% -> 66.39% on epoch=5, global_step=16800
06/21/2023 18:43:58 - INFO - __main__ - Step 17600 Train loss 0.57 Learning rate 4.16e-05 BLEU 65.33% on epoch=5
06/21/2023 18:53:16 - INFO - __main__ - Step 18400 Train loss 0.57 Learning rate 4.12e-05 BLEU 65.75% on epoch=5
06/21/2023 19:02:36 - INFO - __main__ - Step 19200 Train loss 0.57 Learning rate 4.07e-05 BLEU 65.27% on epoch=5
06/21/2023 19:11:51 - INFO - __main__ - Step 20000 Train loss 0.56 Learning rate 4.03e-05 BLEU 65.82% on epoch=6
06/21/2023 19:21:08 - INFO - __main__ - Step 20800 Train loss 0.55 Learning rate 3.99e-05 BLEU 65.29% on epoch=6
06/21/2023 19:30:15 - INFO - __main__ - Step 21600 Train loss 0.56 Learning rate 3.95e-05 BLEU 65.98% on epoch=6
06/21/2023 19:39:15 - INFO - __main__ - Step 22400 Train loss 0.55 Learning rate 3.90e-05 BLEU 66.42% on epoch=6
06/21/2023 19:39:15 - INFO - transformers.configuration_utils - Configuration saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/config.json
06/21/2023 19:39:23 - INFO - transformers.modeling_utils - Model weights saved in ../../../data/dekok/out_normal/jointgt_t5_webnlg_normal/pytorch_model.bin
06/21/2023 19:39:23 - INFO - __main__ - Saving model with best BLEU: 66.39% -> 66.42% on epoch=6, global_step=22400
06/21/2023 19:48:22 - INFO - __main__ - Step 23200 Train loss 0.54 Learning rate 3.86e-05 BLEU 65.40% on epoch=7
06/21/2023 19:57:18 - INFO - __main__ - Step 24000 Train loss 0.54 Learning rate 3.82e-05 BLEU 66.17% on epoch=7
06/21/2023 20:06:19 - INFO - __main__ - Step 24800 Train loss 0.54 Learning rate 3.78e-05 BLEU 65.19% on epoch=7
06/21/2023 20:15:22 - INFO - __main__ - Step 25600 Train loss 0.54 Learning rate 3.74e-05 BLEU 65.89% on epoch=7
06/21/2023 20:24:25 - INFO - __main__ - Step 26400 Train loss 0.53 Learning rate 3.69e-05 BLEU 65.11% on epoch=8
06/21/2023 20:33:34 - INFO - __main__ - Step 27200 Train loss 0.52 Learning rate 3.65e-05 BLEU 65.82% on epoch=8
06/21/2023 20:42:22 - INFO - __main__ - Step 28000 Train loss 0.53 Learning rate 3.61e-05 BLEU 65.67% on epoch=8
06/21/2023 20:50:43 - INFO - __main__ - Step 28800 Train loss 0.52 Learning rate 3.57e-05 BLEU 65.73% on epoch=8
06/21/2023 20:59:09 - INFO - __main__ - Step 29600 Train loss 0.52 Learning rate 3.53e-05 BLEU 65.76% on epoch=9
06/21/2023 21:07:25 - INFO - __main__ - Step 30400 Train loss 0.50 Learning rate 3.48e-05 BLEU 65.99% on epoch=9
